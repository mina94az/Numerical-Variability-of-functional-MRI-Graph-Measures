{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U kaleido\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import significantdigits as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove failed subjects\n",
    "# subjects_to_remove =  ['sub-#', 'sub-##', 'sub-###']\n",
    "# hcsubjects_to_remove=['sub-####', 'sub-#####']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data function for each threshold and removing subject failed in preprocessing\n",
    "def Read_data(threshold):\n",
    "    #ReadWconf data\n",
    "    df=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableWConf_batch_1.pkl')\n",
    "    df1=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableWConf_batch_2.pkl')\n",
    "    df2=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableWConf_batch_3.pkl')\n",
    "    df3=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableWConf_batch_4.pkl')\n",
    "    df4=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableWConf_batch_5.pkl')\n",
    "\n",
    "    all_dfs = [df, df1, df2, df3, df4]\n",
    "    Results_tableWConf = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "    ################################################################################################\n",
    "\n",
    "    #Read NoConf data\n",
    "    dfn=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableNoConf_batch_1.pkl')\n",
    "    dfn1=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableNoConf_batch_2.pkl')\n",
    "    dfn2=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableNoConf_batch_3.pkl')\n",
    "    dfn3=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableNoConf_batch_4.pkl')\n",
    "    dfn4=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableNoConf_batch_5.pkl')\n",
    "\n",
    "    all_dfs = [dfn, dfn1, dfn2, dfn3, dfn4]\n",
    "    Results_tableNoConf = pd.concat(all_dfs, ignore_index=True)  \n",
    "\n",
    "\n",
    "    dfW = Results_tableWConf[~Results_tableWConf['subject'].isin(subjects_to_remove)]\n",
    "    dfN= Results_tableNoConf[~Results_tableNoConf['subject'].isin(subjects_to_remove)]\n",
    "    \n",
    "    ###################################################################################################\n",
    "\n",
    "    #Read healthy control\n",
    "    dfWhc=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableWConf_batch_hc.pkl')\n",
    "    dfNhc=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableNoConf_batch_hc.pkl')\n",
    "    dfWhc = dfWhc[~dfWhc['subject'].isin(hcsubjects_to_remove)]\n",
    "    dfNhc= dfNhc[~dfNhc['subject'].isin(hcsubjects_to_remove)]\n",
    "\n",
    "    return dfW,dfN,dfWhc,dfNhc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW,dfN,dfWhc,dfNhc=Read_data(0.5)\n",
    "alldfss_p=[dfW,dfWhc]\n",
    "all_pop5=pd.concat(alldfss_p,ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW,dfN,dfWhc,dfNhc=Read_data(0.4)\n",
    "alldfss_g=[dfW,dfWhc]\n",
    "all_pop4=pd.concat(alldfss_g,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW,dfN,dfWhc,dfNhc=Read_data(0.3)\n",
    "alldfss_s=[dfW,dfWhc]\n",
    "all_pop3=pd.concat(alldfss_s,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW,dfN,dfWhc,dfNhc=Read_data(0.2)\n",
    "alldfss_d=[dfW,dfWhc]\n",
    "all_pop2=pd.concat(alldfss_d,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW,dfN,dfWhc,dfNhc=Read_data(0.1)\n",
    "alldfss_o=[dfW,dfWhc]\n",
    "all_pop1=pd.concat(alldfss_o,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW,dfN,dfWhc,dfNhc=Read_data(0.05)\n",
    "alldfss_ps=[dfW,dfWhc]\n",
    "all_pop05=pd.concat(alldfss_ps,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the variance across MCA repetitions for each subject, session, and acquisition\n",
    "def makestdevNum_metric(df):\n",
    "    \"\"\"\n",
    "    Calculate standard deviation across MCA repetitions for each subject, session, and acquisition.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing network metrics for different subjects, sessions, acquisitions and repetitions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with standard deviations of metrics for each subject, session, and acquisition\n",
    "    \"\"\"\n",
    "    # Define columns for metrics\n",
    "    metric_columns = [\n",
    "        'degree_(numericalVar)', 'betweeness_(numericalVar)', \n",
    "          'eigenvec_(numericalVar)',  'clusteringcoef_(numericalVar)',\n",
    "        'smallworldness(numericalVar)', 'avg_shortestPathLength(numericalVar)'\n",
    "    ]\n",
    "    \n",
    "    # Define info columns\n",
    "    info_columns = ['subject', 'session', 'acquisition']\n",
    "    \n",
    "    # Pre-filter to exclude unwanted acquisitions for efficiency\n",
    "    df_filtered = df[~df['acquisition'].isin(['acq-RLsplit1', 'acq-LRsplit1'])]\n",
    "    \n",
    "    # Create empty list to collect results\n",
    "    results = []\n",
    "    \n",
    "    # Group by subject, session, acquisition to avoid nested loops\n",
    "    for (subj, session, acquisition), group in df_filtered.groupby(['subject', 'session', 'acquisition']):\n",
    "        # Collections for each metric across repetitions\n",
    "        all_metrics = {\n",
    "            'degree': [], 'betweeness': [], 'eigen': [], \n",
    "            'clustering': [], 'smallworld': [], 'shortestpath': []\n",
    "        }\n",
    "        \n",
    "        # Collect metrics for all repetitions\n",
    "        for _, row in group.iterrows():\n",
    "            rep_num = row.get('repetition')\n",
    "            if not isinstance(rep_num, str) or not rep_num.startswith('rep-'):\n",
    "                continue\n",
    "                \n",
    "            # Only process if we have the necessary data\n",
    "            if (isinstance(row.get('degree_centralities'), dict) and \n",
    "                isinstance(row.get('betweenness_centralities'), dict) and\n",
    "                isinstance(row.get('eigenvector_centralities'), dict) and\n",
    "                isinstance(row.get('clustering_coefficients'), dict)):\n",
    "                \n",
    "                # Extract metrics\n",
    "                all_metrics['degree'].append(list(row['degree_centralities'].values()))\n",
    "                all_metrics['betweeness'].append(list(row['betweenness_centralities'].values()))\n",
    "                all_metrics['eigen'].append(list(row['eigenvector_centralities'].values()))\n",
    "                all_metrics['clustering'].append(list(row['clustering_coefficients'].values()))\n",
    "                \n",
    "                # Add scalar metrics if they exist\n",
    "                if 'small_worldness' in row:\n",
    "                    all_metrics['smallworld'].append(row['small_worldness'])\n",
    "                if 'avg_shortest_path_length' in row:\n",
    "                    all_metrics['shortestpath'].append(row['avg_shortest_path_length'])\n",
    "        \n",
    "        # Only calculate stats if we have data\n",
    "        if not all_metrics['degree']:\n",
    "            continue\n",
    "            \n",
    "        # Calculate standard deviations\n",
    "        result_row = {\n",
    "            'subject': subj,\n",
    "            'session': session,\n",
    "            'acquisition': acquisition,\n",
    "            'degree_(numericalVar)': [np.var(all_metrics['degree'], axis=0)],\n",
    "            'betweeness_(numericalVar)': [np.var(all_metrics['betweeness'], axis=0)],\n",
    "            'eigenvec_(numericalVar)': [np.var(all_metrics['eigen'], axis=0)],\n",
    "            'clusteringcoef_(numericalVar)': [np.var(all_metrics['clustering'], axis=0)]\n",
    "        }\n",
    "        \n",
    "        # Add scalar metrics if available\n",
    "        if all_metrics['smallworld']:\n",
    "            result_row['smallworldness(numericalVar)'] = np.var(all_metrics['smallworld'], axis=0)\n",
    "        if all_metrics['shortestpath']:\n",
    "            result_row['avg_shortestPathLength(numericalVar)'] = np.var(all_metrics['shortestpath'], axis=0)\n",
    "            \n",
    "        results.append(result_row)\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    avrofStd_WithinSubject = pd.DataFrame(results)\n",
    "    \n",
    "    # Ensure proper data types for object columns\n",
    "    for col in metric_columns:\n",
    "        if col in avrofStd_WithinSubject.columns:\n",
    "            avrofStd_WithinSubject[col] = avrofStd_WithinSubject[col].astype('object')\n",
    "    \n",
    "    return avrofStd_WithinSubject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the variance across subjects for each repetition, session, and acquisition\n",
    "def makestedvAnat_metric(df):\n",
    "    \"\"\"\n",
    "    Calculate standard deviation of network metrics across subjects for each repetition, session, and acquisition.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing network metrics for different subjects, sessions, acquisitions and repetitions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with standard deviations of metrics for each repetition, session, and acquisition\n",
    "    \"\"\"\n",
    "    # Define columns for metrics\n",
    "    metric_columns = [\n",
    "        'degree_(AnatomicalVar)', 'betweeness_(AnatomicalVar)', \n",
    "         'eigenvec_(AnatomicalVar)', 'clusteringcoef_(AnatomicalVar)', \n",
    "        'smallworldness(AnatomicalVar)', 'avg_shortestPathLength(AnatomicalVar)'\n",
    "    ]\n",
    "    \n",
    "    # Info columns\n",
    "    info_columns = ['iteration', 'session', 'acquisition']\n",
    "    \n",
    "    # Pre-filter to exclude unwanted acquisitions for efficiency\n",
    "    df_filtered = df[~df['acquisition'].isin(['acq-RLsplit1', 'acq-LRsplit1'])]\n",
    "    \n",
    "    # Create empty list to collect results\n",
    "    results = []\n",
    "    \n",
    "    # First loop through repetitions (since we want to group by repetition)\n",
    "    for rep in range(1, 11):\n",
    "        rep_str = f'rep-{rep}'\n",
    "        # Filter for current repetition\n",
    "        rep_df = df_filtered[df_filtered['repetition'] == rep_str]\n",
    "        \n",
    "        if rep_df.empty:\n",
    "            continue\n",
    "            \n",
    "        # Group by session and acquisition\n",
    "        for (session, acquisition), group in rep_df.groupby(['session', 'acquisition']):\n",
    "            # Collections for metrics across subjects\n",
    "            all_metrics = {\n",
    "                'degree': [], 'betweeness': [], 'eigen': [], \n",
    "                'clustering': [], 'smallworld': [], 'shortestpath': []\n",
    "            }\n",
    "            \n",
    "            # Loop through subjects in this group\n",
    "            for subject, subject_data in group.groupby('subject'):\n",
    "                if subject_data.empty:\n",
    "                    continue\n",
    "                    \n",
    "                # Get the first (and should be only) row for this subject in this repetition/session/acquisition\n",
    "                row = subject_data.iloc[0]\n",
    "                \n",
    "                # Check if required data exists\n",
    "                if (isinstance(row.get('degree_centralities'), dict) and \n",
    "                    isinstance(row.get('betweenness_centralities'), dict) and\n",
    "                    isinstance(row.get('eigenvector_centralities'), dict) and\n",
    "                    isinstance(row.get('clustering_coefficients'), dict)):\n",
    "                    \n",
    "                    # Extract metrics\n",
    "                    all_metrics['degree'].append(list(row['degree_centralities'].values()))\n",
    "                    all_metrics['betweeness'].append(list(row['betweenness_centralities'].values()))\n",
    "                    all_metrics['eigen'].append(list(row['eigenvector_centralities'].values()))\n",
    "                    all_metrics['clustering'].append(list(row['clustering_coefficients'].values()))\n",
    "                    \n",
    "                    # Add scalar metrics if they exist\n",
    "                    if 'small_worldness' in row:\n",
    "                        all_metrics['smallworld'].append(row['small_worldness'])\n",
    "                    if 'avg_shortest_path_length' in row:\n",
    "                        all_metrics['shortestpath'].append(row['avg_shortest_path_length'])\n",
    "            # if((session=='1') & (acquisition=='acq-RL')):\n",
    "            #     print(len(all_metrics['degree']))\n",
    "            # Only calculate stats if we have enough data (at least 2 subjects)\n",
    "            if len(all_metrics['degree']) < 2:\n",
    "                continue\n",
    "            # if ((session=='1') & (acquisition=='acq-RL')):\n",
    "            #     print(len(all_metrics['degree']))\n",
    "            # Calculate standard deviations\n",
    "            result_row = {\n",
    "                'iteration': f'iter_{rep}',\n",
    "                'session': session,\n",
    "                'acquisition': acquisition,\n",
    "                'degree_(AnatomicalVar)': [np.var(all_metrics['degree'], axis=0)],\n",
    "                'betweeness_(AnatomicalVar)': [np.var(all_metrics['betweeness'], axis=0)],\n",
    "                # Note: Fixed a bug in the original code - eigenvec and clusteringcoef were swapped\n",
    "                'eigenvec_(AnatomicalVar)': [np.var(all_metrics['eigen'], axis=0)],\n",
    "                'clusteringcoef_(AnatomicalVar)': [np.var(all_metrics['clustering'], axis=0)]\n",
    "            }\n",
    "            \n",
    "            # Add scalar metrics if available\n",
    "            if all_metrics['smallworld']:\n",
    "                result_row['smallworldness(AnatomicalVar)'] = np.var(all_metrics['smallworld'], axis=0)\n",
    "            if all_metrics['shortestpath']:\n",
    "                result_row['avg_shortestPathLength(AnatomicalVar)'] = np.var(all_metrics['shortestpath'], axis=0)\n",
    "                \n",
    "            results.append(result_row)\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    avrofStd_BetweenSubject = pd.DataFrame(results)\n",
    "    \n",
    "    # Ensure proper data types for object columns\n",
    "    for col in metric_columns:\n",
    "        if col in avrofStd_BetweenSubject.columns:\n",
    "            avrofStd_BetweenSubject[col] = avrofStd_BetweenSubject[col].astype('object')\n",
    "    \n",
    "    return avrofStd_BetweenSubject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df5_Wconf_num=makestdevNum_metric(all_pop5)\n",
    "df5_Wconf_anat=makestedvAnat_metric(all_pop5)\n",
    "\n",
    "#######################################\n",
    "df4_Wconf_num=makestdevNum_metric(all_pop4)\n",
    "df4_Wconf_anat=makestedvAnat_metric(all_pop4)\n",
    "\n",
    "################################################\n",
    "df3_Wconf_num=makestdevNum_metric(all_pop3)\n",
    "df3_Wconf_anat=makestedvAnat_metric(all_pop3)\n",
    "\n",
    "\n",
    "##########################################################\n",
    "df2_Wconf_num=makestdevNum_metric(all_pop2)\n",
    "df2_Wconf_anat=makestedvAnat_metric(all_pop2)\n",
    "\n",
    "\n",
    "#########################################################\n",
    "\n",
    "df1_Wconf_num=makestdevNum_metric(all_pop1)\n",
    "df1_Wconf_anat=makestedvAnat_metric(all_pop1)\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "df05_Wconf_num=makestdevNum_metric(all_pop05)\n",
    "df05_Wconf_anat=makestedvAnat_metric(all_pop05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the root square of the average of variances across MCA runs and subjects\n",
    "\n",
    "def compute_average_anat(df_Noconf_anat, df_Wconf_anat):\n",
    "    \"\"\"\n",
    "    Compute average network metrics for 'no confound' and 'with confound' conditions across 10 MCA runs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_Noconf_anat : pandas.DataFrame\n",
    "        DataFrame containing anatomical network metrics for 'no confound' condition\n",
    "    df_Wconf_anat : pandas.DataFrame\n",
    "        DataFrame containing anatomical network metrics for 'with confound' condition\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of pandas.DataFrame\n",
    "        Two DataFrames containing the averaged network metrics for both conditions\n",
    "    \"\"\"\n",
    "    # Define columns to process\n",
    "    metric_cols = [\n",
    "        'degree_(AnatomicalVar)', \n",
    "        'betweeness_(AnatomicalVar)', \n",
    "        'eigenvec_(AnatomicalVar)', \n",
    "        'clusteringcoef_(AnatomicalVar)', \n",
    "        'smallworldness(AnatomicalVar)',\n",
    "        'avg_shortestPathLength(AnatomicalVar)'\n",
    "    ]\n",
    "    \n",
    "    # Process a single dataframe\n",
    "    def process_dataframe(df, rename_cols=False):\n",
    "        # Filter the dataframe\n",
    "        filtered_df = df[(df['acquisition'] == 'acq-RL') & \n",
    "                         (df['session'] == '1')] #&(df['iteration'] == \"iter_1\")]\n",
    "        # print(len(filtered_df))\n",
    "        # Dictionary to store stacked data\n",
    "        stacked_data = {}\n",
    "        \n",
    "        # Process each metric column\n",
    "        for col in metric_cols:\n",
    "            stacked_data[col] = np.vstack(filtered_df[col].values)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        stacked_df = pd.DataFrame([stacked_data])\n",
    "        \n",
    "        result_df = pd.DataFrame({\n",
    "            col: [np.sqrt(np.mean(stacked_df.iloc[0][col], axis=0))] for col in metric_cols\n",
    "        })\n",
    "        \n",
    "        # # Add scalar metrics directly\n",
    "        # result_df['smallworldness(AnatomicalVar)'] = stacked_df['smallworldness(AnatomicalVar)']\n",
    "        # result_df['avg_shortestPathLength(AnatomicalVar)'] = stacked_df['avg_shortestPathLength(AnatomicalVar)']\n",
    "        # âœ… check if any NaN exists in the DataFrame\n",
    "        if result_df.isnull().values.any(): \n",
    "            print(\"âš ï¸ Warning: result_df contains NaN values in some columns\")\n",
    "            # optionally, print which columns\n",
    "            print(\"Columns with NaN:\", result_df.columns[result_df.isnull().any()].tolist())\n",
    "  \n",
    "        # Rename columns if requested (for Wconf data)\n",
    "        if rename_cols:\n",
    "            rename_mapping = {col: col.replace('(AnatomicalVar)', '(AnatomicalVarW)') \n",
    "                             for col in result_df.columns}\n",
    "            result_df = result_df.rename(columns=rename_mapping)\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    # Process both dataframes\n",
    "    avrg_Noconf_anat = process_dataframe(df_Noconf_anat, rename_cols=False)\n",
    "    avrg_Wconf_anat = process_dataframe(df_Wconf_anat, rename_cols=True)\n",
    "    \n",
    "    return avrg_Noconf_anat, avrg_Wconf_anat\n",
    "\n",
    "\n",
    "def compute_average_num(df_Noconf_num, df_Wconf_num):\n",
    "    \"\"\"\n",
    "    Compute average numerical network metrics for 'no confidence' and 'with confidence' conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_Noconf_num : pandas.DataFrame\n",
    "        DataFrame containing numerical network metrics for 'no confidence' condition\n",
    "    df_Wconf_num : pandas.DataFrame\n",
    "        DataFrame containing numerical network metrics for 'with confidence' condition\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of pandas.DataFrame\n",
    "        Two DataFrames containing the averaged numerical metrics for both conditions\n",
    "    \"\"\"\n",
    "    # Define columns to process\n",
    "    metric_cols = [\n",
    "        'degree_(numericalVar)', \n",
    "        'betweeness_(numericalVar)', \n",
    "        'eigenvec_(numericalVar)', \n",
    "        'clusteringcoef_(numericalVar)', \n",
    "        'smallworldness(numericalVar)',\n",
    "        'avg_shortestPathLength(numericalVar)'\n",
    "    ]\n",
    "    \n",
    "    # Process a single dataframe\n",
    "    def process_dataframe(df, rename_cols=False):\n",
    "        # Filter the dataframe\n",
    "        filtered_df = df[(df['acquisition'] == 'acq-RL') & \n",
    "                         (df['session'] == '1')]\n",
    "        print(len(filtered_df))\n",
    "        # Dictionary to store stacked data\n",
    "        stacked_data = {}\n",
    "        \n",
    "        # Process each metric column\n",
    "        for col in metric_cols:\n",
    "            stacked_data[col] = np.vstack(filtered_df[col].values)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        stacked_df = pd.DataFrame([stacked_data])\n",
    "        \n",
    "\n",
    "        \n",
    "        result_df = pd.DataFrame({\n",
    "            col: [np.sqrt(np.mean(stacked_df.iloc[0][col], axis=0))] for col in metric_cols\n",
    "        })\n",
    "                # âœ… check if any NaN exists in the DataFrame\n",
    "        if result_df.isnull().values.any(): \n",
    "            print(\"âš ï¸ Warning: result_df contains NaN values in some columns\")\n",
    "            # optionally, print which columns\n",
    "            print(\"Columns with NaN:\", result_df.columns[result_df.isnull().any()].tolist())\n",
    "        # # Add scalar metrics directly\n",
    "        # result_df['smallworldness(numericalVar)'] = stacked_df['smallworldness(numericalVar)']\n",
    "        # result_df['avg_shortestPathLength(numericalVar)'] = stacked_df['avg_shortestPathLength(numericalVar)']\n",
    "        \n",
    "        # Rename columns if requested (for Wconf data)\n",
    "        if rename_cols:\n",
    "            rename_mapping = {col: col.replace('(numericalVar)', '(numericalVarW)') \n",
    "                             for col in result_df.columns}\n",
    "            result_df = result_df.rename(columns=rename_mapping)\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    # Process both dataframes\n",
    "    avrg_Noconf_num = process_dataframe(df_Noconf_num, rename_cols=False)\n",
    "    avrg_Wconf_num = process_dataframe(df_Wconf_num, rename_cols=True)\n",
    "    \n",
    "    return avrg_Noconf_num, avrg_Wconf_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Standard deviation across MCA runs (for each subject, region, and metric):\n",
    "\n",
    "$$\n",
    "\\sigma_{s,r,m} = \\mathrm{StdDev}(X_{s,r,m,1}, X_{s,r,m,2}, \\ldots, X_{s,r,m,10})\n",
    "$$\n",
    "\n",
    "Average standard deviation across subjects (per region and metric):\n",
    "\n",
    "$$\n",
    "\\overline{\\sigma}_{r, m} = \\frac{1}{S} \\sum_{s=1}^{S} \\sigma_{s, r, m}\n",
    "$$\n",
    "Standard deviation across subjects (for each MCA iteration, region, and metric):\n",
    "\n",
    "$$\\sigma_{r,m,iter} = \\mathrm{StdDev}(X_{1,r,m,iter}, X_{2,r,m,iter}, \\ldots, X_{s,r,m,iter})$$\n",
    "\n",
    "Average standard deviation across MCA iteration (per region and metric):\n",
    "\n",
    "$$\\overline{\\sigma}_{r, m} = \\frac{1}{10} \\sum_{iter=1}^{10} \\sigma_{ r, m,iter}$$\n",
    "\n",
    "$$s \\in \\{Subjects\\} , r \\in \\{Regions\\}, m \\in \\{ Metrics \\}, iter \\in \\{1,...,10\\} $$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, avrg1_Wconf_num = compute_average_num(df1_Wconf_num, df1_Wconf_num)\n",
    "_, avrg1_Wconf_anat = compute_average_anat(df1_Wconf_anat, df1_Wconf_anat)\n",
    "\n",
    "\n",
    "\n",
    "_, avrg2_Wconf_num = compute_average_num(df2_Wconf_num, df2_Wconf_num)\n",
    "_, avrg2_Wconf_anat = compute_average_anat(df2_Wconf_anat, df2_Wconf_anat)\n",
    "\n",
    "\n",
    "_, avrg3_Wconf_num = compute_average_num(df3_Wconf_num, df3_Wconf_num)\n",
    "_, avrg3_Wconf_anat = compute_average_anat(df3_Wconf_anat, df3_Wconf_anat)\n",
    "\n",
    "\n",
    "\n",
    "_, avrg4_Wconf_num = compute_average_num(df4_Wconf_num, df4_Wconf_num)\n",
    "_, avrg4_Wconf_anat = compute_average_anat(df4_Wconf_anat, df4_Wconf_anat)\n",
    "\n",
    "\n",
    "\n",
    "_, avrg5_Wconf_num = compute_average_num(df5_Wconf_num, df5_Wconf_num)\n",
    "_, avrg5_Wconf_anat = compute_average_anat(df5_Wconf_anat, df5_Wconf_anat)\n",
    "\n",
    "\n",
    "\n",
    "_, avrg05_Wconf_num = compute_average_num(df05_Wconf_num, df05_Wconf_num)\n",
    "_, avrg05_Wconf_anat = compute_average_anat(df05_Wconf_anat, df05_Wconf_anat)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "atlas = datasets.fetch_atlas_schaefer_2018(n_rois=100, yeo_networks=7)\n",
    "atlas_labels = atlas.labels[1:]\n",
    "atlas_filename=atlas.maps\n",
    "# atlas_labels[0:51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the NPVR for global metrics\n",
    "def cmput_ratio_g(df, df1, N):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    metrics = ['smallworldness', 'avg_shortestPathLength']\n",
    "    # numerical_data = {'subject': df['subject']}\n",
    "    numerical_data = {}\n",
    "    for metric in metrics:\n",
    "        numerical_col = [col for col in df.columns if metric in col and 'numericalVar' in col]\n",
    "        anatomical1_col = [col for col in df1.columns if metric in col and 'AnatomicalVar' in col]\n",
    "\n",
    "        if numerical_col and anatomical1_col :\n",
    "            numerical_col = numerical_col[0]\n",
    "            anatomical_col1 = anatomical1_col[0]\n",
    "\n",
    "            a = df1[anatomical_col1].values[0]  # make sure this is a scalar\n",
    "            # n=(df[numerical_col])\n",
    "            n=(df[numerical_col]).values[0]\n",
    "            ratio_values =((n) / (a)) if a != 0 else print('zero') #(2/np.sqrt(N)) *\n",
    "            numerical_data[f'ratio_{metric}'] = ratio_values\n",
    "        else:\n",
    "            print(f\"Warning: Could not find matching columns for {metric}\")\n",
    "\n",
    "    result_df = pd.DataFrame(numerical_data)\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the NPVR for local metrics\n",
    "\n",
    "def cmput_ratio(df,df1,N):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Dictionary to hold column: list_of_ratios\n",
    "    ratio_data = {}\n",
    "\n",
    "    # List of metric keywords to match columns\n",
    "    metrics = ['degree', 'betweeness', 'eigenvec', 'clusteringcoef']\n",
    "\n",
    "    for metric in metrics:\n",
    "        # Get the matching column names\n",
    "        num_col = [col for col in df.columns if metric in col][0]\n",
    "        anat1_col = [col for col in df1.columns if metric in col][0]\n",
    "        # Extract the values (lists of 100 values)\n",
    "        num_vals = df[num_col].iloc[0]\n",
    "        anat_vals = df1[anat1_col].iloc[0]\n",
    "        # N1 = 147-8\n",
    "        # N2 = 34  # assuming equal sizes\n",
    "\n",
    "        # Flatten if nested in [[x]]\n",
    "        if isinstance(num_vals[0], list):\n",
    "            num_vals = [v[0] for v in num_vals]\n",
    "            anat_vals = [v[0] for v in anat_vals]\n",
    "            # print('num_vals=', num_vals)\n",
    "        # Compute Ïƒ_pool for each pair and then n / Ïƒ_pool\n",
    "        pooled_ratios = []\n",
    "        for n, a in zip(num_vals, anat_vals):\n",
    "            ratio =(  ((n) / (a)) )if a != 0 else print('zero') #(2/np.sqrt(N)) *\n",
    "            # print(\"n,a,b\",n,a,b)\n",
    "            pooled_ratios.append(ratio)\n",
    "\n",
    "        # Store as a single list under one row\n",
    "        ratio_data[metric + '_ratio'] = [pooled_ratios]\n",
    "\n",
    "    # Convert to single-row DataFrame\n",
    "    ratio_df = pd.DataFrame(ratio_data)\n",
    "\n",
    "    return ratio_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- $$\n",
    "\\bar{\\sigma}(\\text{d}) = \\frac{\\bar{\\sigma}(\\text{num})}{\\bar{\\sigma}_{pool}}\n",
    "$$ -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Variance across MCA runs (for each subject, region, and metric):\n",
    "\n",
    "$$\n",
    "Var_{s,r,m} = \\mathrm{Var}(X_{s,r,m,1}, X_{s,r,m,2}, \\ldots, X_{s,r,m,10})\n",
    "$$\n",
    "\n",
    "Average Variance across subjects (per region and metric):\n",
    "\n",
    "$$\n",
    "\\overline{Var}_{r, m} = \\frac{1}{S} \\sum_{s=1}^{S} Var_{s, r, m}\n",
    "$$\n",
    "$$\n",
    "\\sigma_{num} = \\sqrt{\\overline{Var}_{r, m}}\n",
    "$$\n",
    "\n",
    "Variance  across subjects (for each MCA iteration, region, and metric):\n",
    "\n",
    "$$Var_{r,m,iter} = \\mathrm{Var}(X_{1,r,m,iter}, X_{2,r,m,iter}, \\ldots, X_{s,r,m,iter})$$\n",
    "\n",
    "Average Variance across MCA iteration (per region and metric):\n",
    "\n",
    "$$\\overline{Var}_{r, m} = \\frac{1}{10} \\sum_{iter=1}^{10} Var_{ r, m,iter}$$\n",
    "\n",
    "$$\n",
    "\\sigma_{pool} = \\sqrt{\\overline{Var}_{r, m}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$s \\in \\{Subjects\\} , r \\in \\{Regions\\}, m \\in \\{ Metrics \\}, iter \\in \\{1,...,10\\} $$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio1=cmput_ratio(avrg1_Wconf_num,avrg1_Wconf_anat,182)\n",
    "ratio1G=cmput_ratio_g(avrg1_Wconf_num ,avrg1_Wconf_anat,182)\n",
    "\n",
    "ratio2=cmput_ratio(avrg2_Wconf_num,avrg2_Wconf_anat,179)\n",
    "ratio2G=cmput_ratio_g(avrg2_Wconf_num ,avrg2_Wconf_anat,179)\n",
    "\n",
    "ratio3=cmput_ratio(avrg3_Wconf_num,avrg3_Wconf_anat,176)\n",
    "ratio3G=cmput_ratio_g(avrg3_Wconf_num ,avrg3_Wconf_anat,176)\n",
    "\n",
    "ratio4=cmput_ratio(avrg4_Wconf_num,avrg4_Wconf_anat,170)\n",
    "ratio4G=cmput_ratio_g(avrg4_Wconf_num ,avrg4_Wconf_anat,170)\n",
    "\n",
    "ratio5=cmput_ratio(avrg5_Wconf_num,avrg5_Wconf_anat,147)\n",
    "ratio5G=cmput_ratio_g(avrg5_Wconf_num ,avrg5_Wconf_anat,147)\n",
    "\n",
    "ratio05=cmput_ratio(avrg05_Wconf_num,avrg05_Wconf_anat,182)\n",
    "ratio05G=cmput_ratio_g(avrg05_Wconf_num ,avrg05_Wconf_anat,182)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the average NPVR for local and global metrics\n",
    "avrgRatio05 = {}\n",
    "avrgRatio1 = {}\n",
    "avrgRatio2 = {}\n",
    "avrgRatio3= {}\n",
    "avrgRatio4 = {}\n",
    "avrgRatio5= {}\n",
    "\n",
    "\n",
    "for col in ratio1.columns:\n",
    "    avrgRatio05[col]=np.mean(ratio05[col].values[0])\n",
    "    avrgRatio1[col]=np.mean(ratio1[col].values[0])\n",
    "    avrgRatio2[col]=np.mean(ratio2[col].values[0])\n",
    "    avrgRatio3[col]=np.mean(ratio3[col].values[0])\n",
    "    avrgRatio4[col]=np.mean(ratio4[col].values[0])\n",
    "    avrgRatio5[col]=np.mean(ratio5[col].values[0])\n",
    "\n",
    "\n",
    "GavrgRatio05 = {}\n",
    "GavrgRatio1 = {}\n",
    "GavrgRatio2 = {}\n",
    "GavrgRatio3= {}\n",
    "GavrgRatio4 = {}\n",
    "GavrgRatio5= {}\n",
    "\n",
    "for col in ratio1G.columns[1:]:\n",
    "    GavrgRatio05[col]=np.mean(list(ratio05G[col]))\n",
    "    GavrgRatio1[col]=np.mean(list(ratio1G[col]))\n",
    "    GavrgRatio2[col]=np.mean(list(ratio2G[col]))\n",
    "    GavrgRatio3[col]=np.mean(list(ratio3G[col]))\n",
    "    GavrgRatio4[col]=np.mean(list(ratio4G[col]))\n",
    "    GavrgRatio5[col]=np.mean(list(ratio5G[col]))\n",
    "avrgRatio05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Compute ratios\n",
    "ratio1 = cmput_ratio(avrg1_Wconf_num, avrg1_Wconf_anat, 182)\n",
    "ratio2 = cmput_ratio(avrg2_Wconf_num, avrg2_Wconf_anat, 179)\n",
    "ratio3 = cmput_ratio(avrg3_Wconf_num, avrg3_Wconf_anat, 176)\n",
    "ratio4 = cmput_ratio(avrg4_Wconf_num, avrg4_Wconf_anat, 170)\n",
    "ratio5 = cmput_ratio(avrg5_Wconf_num, avrg5_Wconf_anat, 147)\n",
    "ratio05 = cmput_ratio(avrg05_Wconf_num, avrg05_Wconf_anat, 182)\n",
    "\n",
    "\n",
    "def change(arr,col):\n",
    "    # Create the 'region' column with labels\n",
    "    regions = [f\"region_{i+1}\" for i in range(len(arr))]\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({'region': regions, f'{col}': arr})\n",
    "    return df\n",
    "# Dictionary mapping threshold digits -> ratio DataFrame\n",
    "ratio_map = {\n",
    "    \"5\": ratio5,\n",
    "    \"4\": ratio4,\n",
    "    \"3\": ratio3,\n",
    "    \"2\": ratio2,\n",
    "    \"1\": ratio1,\n",
    "    \"05\": ratio05\n",
    "}\n",
    "\n",
    "# Thresholds\n",
    "thresholds = [0.5, 0.4, 0.3, 0.2, 0.1, 0.05]  \n",
    "\n",
    "# Metrics\n",
    "metrics = ['betweeness', 'clusteringcoef', 'eigen', 'degree']\n",
    "metric_labels = [\"Betweenness Centrality\", \"Clustering Coefficient\",\n",
    "                 \"Eigenvector Centrality\", \"Degree Centrality\"]\n",
    "\n",
    "# Colors\n",
    "colors = {\"PD\": \"rgb(128, 0, 128)\"}\n",
    "\n",
    "# Subplot grid: 4 rows (metrics) Ã— len(thresholds) cols\n",
    "fig = make_subplots(\n",
    "    rows=len(metrics), cols=len(thresholds),\n",
    "    shared_yaxes='rows',\n",
    "    subplot_titles=[f\"Thr {thr}\" for thr in thresholds]*len(metrics),\n",
    "    vertical_spacing=0.08\n",
    ")\n",
    "\n",
    "# Loop over metrics & thresholds\n",
    "for row, (metric, metric_label) in enumerate(zip(metrics, metric_labels), start=1):\n",
    "    for col, thr in enumerate(thresholds, start=1):\n",
    "        thre = str(thr).split(\".\")[1]   # e.g. 0.05 -> \"05\", 0.5 -> \"5\"\n",
    "        rPD = ratio_map[thre]\n",
    "        # --- Extract values for this metric ---\n",
    "        pd_val = rPD[[c for c in rPD.columns if metric in c]].iloc[0, 0]\n",
    "        \n",
    "        pd_valreg=change(pd_val[:],metric)\n",
    "        pd_vals=pd_valreg.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "        # print(thre,metric,pd_vals)\n",
    "        # Add PD box\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=pd_vals['value'],\n",
    "                name=\"Sample\",\n",
    "                marker=dict(color=colors[\"PD\"], size=4),\n",
    "                boxpoints=\"all\", jitter=0.3, pointpos=-0.4,\n",
    "                width=0.35,\n",
    "                text=pd_vals['region'],\n",
    "                hovertemplate=\"<b>Region:</b> %{text}<br><b>Value:</b> %{y}<br><extra>PD</extra>\",\n",
    "                showlegend=(row==1 and col==1)  # only show legend once\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "    # Add row title (metric label) on y-axis\n",
    "    fig.update_yaxes(title_text=metric_label, row=row, col=1)\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    height=300*len(metrics),\n",
    "    width=220*len(thresholds),\n",
    "    title=\"Sample variability Ratio across different metric and absolute  thresholds\",\n",
    "    boxmode=\"group\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import re\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "def change(arr, col):\n",
    "    regions = [f\"region_{i+1}\" for i in range(len(arr))]\n",
    "    df = pd.DataFrame({'region': regions, f'{col}': arr})\n",
    "    return df\n",
    "# ---- Setup ----\n",
    "metrics = ['degree', 'clusteringcoef', 'eigenvec', 'betweeness']\n",
    "metric_labels = [\"Degree Centrality\", \"Clustering Coefficient\", \"EigenVector Centrality\", \"Betweeness Centrality\"]\n",
    "thresholds = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Data for num and anat\n",
    "dataframes_num = [avrg05_Wconf_num, avrg1_Wconf_num, avrg2_Wconf_num, avrg3_Wconf_num, avrg4_Wconf_num, avrg5_Wconf_num]\n",
    "dataframes_anat = [avrg05_Wconf_anat, avrg1_Wconf_anat, avrg2_Wconf_anat, avrg3_Wconf_anat, avrg4_Wconf_anat, avrg5_Wconf_anat]\n",
    "dataframes_avrRatio=[avrgRatio05, avrgRatio1, avrgRatio2, avrgRatio3, avrgRatio4, avrgRatio5]\n",
    "\n",
    "\n",
    "colors_num = px.colors.sequential.Blues[1:7]   # red spectrum\n",
    "colors_anat = px.colors.sequential.Oranges[1:7] # blue spectrum\n",
    "# ---- Calculate global min/max for each metric ----\n",
    "metric_ranges = {}\n",
    "for metric in metrics:\n",
    "    all_values = []\n",
    "    \n",
    "    for threshold, df_num, df_anat, df_avr in zip(thresholds, dataframes_num, dataframes_anat, dataframes_avrRatio):\n",
    "        for df in [df_num, df_anat]:\n",
    "            columns_to_extract_pd = [col_name for col_name in df.columns if metric in col_name]\n",
    "            if columns_to_extract_pd:\n",
    "                statics_pd = change(df[columns_to_extract_pd].iloc[0][columns_to_extract_pd[0]], columns_to_extract_pd[0])\n",
    "                melted_pd = statics_pd.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "                all_values.extend(melted_pd['value'].tolist())\n",
    "\n",
    "        # also include average ratio values\n",
    "        matching_keys = [k for k in df_avr.keys() if metric in k]\n",
    "        for k in matching_keys:\n",
    "            val = df_avr[k]\n",
    "            if not np.isnan(val):\n",
    "                all_values.append(val)\n",
    "\n",
    "    if all_values:\n",
    "        y_min = min(all_values)\n",
    "        y_max = max(all_values)\n",
    "        padding = (y_max - y_min) * 0.1\n",
    "        metric_ranges[metric] = (y_min - padding, y_max + padding)\n",
    "\n",
    "# ---- Create subplot with secondary y-axis enabled ----\n",
    "fig_combined = make_subplots(\n",
    "    rows=4, cols=1,\n",
    "    vertical_spacing=0.04,\n",
    "    shared_xaxes=False,\n",
    "    shared_yaxes=False,\n",
    "    subplot_titles=[f\"<b>{label}</b>\" for label in metric_labels],\n",
    "    \n",
    "    specs=[[{\"secondary_y\": True}] for _ in range(4)]  # ðŸ”‘ enable secondary y\n",
    ")\n",
    "# Track if we've added legend items\n",
    "legend_added = {'num': False, 'anat': False, 'ratio': False, 'trend': False}\n",
    "\n",
    "# ---- Process and plot each metric ----\n",
    "for row, (metric, metric_label) in enumerate(zip(metrics, metric_labels)):\n",
    "    \n",
    "    avg_ratio_values = []\n",
    "    x_positions = []\n",
    "    \n",
    "    for col_idx, (threshold, df_num, df_anat, df_avr) in enumerate(zip(thresholds, dataframes_num, dataframes_anat, dataframes_avrRatio)):\n",
    "        \n",
    "        group_center = col_idx * 3\n",
    "        x_num = group_center - 0.4\n",
    "        x_anat = group_center + 0.4\n",
    "        x_star = group_center\n",
    "        \n",
    "        matching_keys = [k for k in df_avr.keys() if metric in k]\n",
    "        if matching_keys:\n",
    "            avrg_val = df_avr[matching_keys[0]]\n",
    "            avg_ratio_values.append(avrg_val)\n",
    "            x_positions.append(x_star)\n",
    "            avrg_val_str = f\"{avrg_val:.3f}\"\n",
    "        else:\n",
    "            avrg_val_str = \"N/A\"\n",
    "        \n",
    "        # sigma(num)\n",
    "        columns_to_extract_pd_num = [col_name for col_name in df_num.columns if metric in col_name]\n",
    "        if columns_to_extract_pd_num:\n",
    "            statics_pd_num = change(df_num[columns_to_extract_pd_num].iloc[0][columns_to_extract_pd_num[0]], columns_to_extract_pd_num[0])\n",
    "            melted_pd_num = statics_pd_num.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "            fig_combined.add_trace(\n",
    "                go.Box(\n",
    "                    x=[x_num] * len(melted_pd_num),\n",
    "                    y=melted_pd_num['value'], \n",
    "                    name=\"(Numerical Variability (NV), Ïƒ_num)\" if not legend_added['num'] else \"\",\n",
    "                    boxpoints='all', jitter=0.2, pointpos=-0.3, width=0.6,\n",
    "                    marker=dict(color='blue', size=4),\n",
    "                    legendgroup=\"num\", \n",
    "                    showlegend=(not legend_added['num']),\n",
    "                    text=melted_pd_num['region'],\n",
    "                    hovertemplate=f\"<b>Metric:</b> {metric_label}<br><b>Threshold:</b> {threshold}<br><b>Type:</b> sigma(num)<br><b>Region:</b> %{{text}}<br><b>Value:</b> %{{y}}<br><extra></extra>\"\n",
    "                ),\n",
    "                row=row+1, col=1, secondary_y=False\n",
    "            )\n",
    "            if not legend_added['num']:\n",
    "                legend_added['num'] = True\n",
    "        # sigma(anat)\n",
    "        columns_to_extract_pd_anat = [col_name for col_name in df_anat.columns if metric in col_name]\n",
    "        if columns_to_extract_pd_anat:\n",
    "            statics_pd_anat = change(df_anat[columns_to_extract_pd_anat].iloc[0][columns_to_extract_pd_anat[0]], columns_to_extract_pd_anat[0])\n",
    "            melted_pd_anat = statics_pd_anat.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "            fig_combined.add_trace(\n",
    "                go.Box(\n",
    "                    x=[x_anat] * len(melted_pd_anat),\n",
    "                    y=melted_pd_anat['value'], \n",
    "                    name=\"(Population Variability(PV), Ïƒ_pop)\" if not legend_added['anat'] else \"\",\n",
    "                    boxpoints='all', jitter=0.2, pointpos=-0.3, width=0.6,\n",
    "                    marker=dict(color='orange', size=4),\n",
    "                    legendgroup=\"anat\", \n",
    "                    showlegend=(not legend_added['anat']),\n",
    "                    text=melted_pd_anat['region'],\n",
    "                    hovertemplate=f\"<b>Metric:</b> {metric_label}<br><b>Threshold:</b> {threshold}<br><b>Type:</b> sigma(anat)<br><b>Region:</b> %{{text}}<br><b>Value:</b> %{{y}}<br><extra></extra>\"\n",
    "                ),\n",
    "                row=row+1, col=1, secondary_y=False\n",
    "            )\n",
    "            if not legend_added['anat']:\n",
    "                legend_added['anat'] = True\n",
    "        # Average ratio (star) â€” on secondary axis\n",
    "        if matching_keys:\n",
    "            fig_combined.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[x_star],\n",
    "                    y=[float(avrg_val_str)],\n",
    "                    mode='lines+markers',\n",
    "                    marker=dict(symbol='star', size=20, color='red', line=dict(color='black', width=1)),\n",
    "                    name=\"Average NPVR\" if not legend_added['ratio'] else \"\",\n",
    "                    legendgroup=\"ratio\",\n",
    "                    showlegend=(not legend_added['ratio']),\n",
    "                    hovertemplate=(\n",
    "                        f\"<b>Metric:</b> {metric_label}<br>\"\n",
    "                        f\"<b>Threshold:</b> {threshold}<br>\"\n",
    "                        f\"<b>Avg Ratio:</b> {avrg_val_str}<br><extra></extra>\"\n",
    "                    )\n",
    "                ),\n",
    "                row=row+1, col=1, secondary_y=True\n",
    "            )\n",
    "            if not legend_added['ratio']:\n",
    "                legend_added['ratio'] = True\n",
    "    # Trend line (secondary axis)\n",
    "    if len(avg_ratio_values) > 1:\n",
    "        fig_combined.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_positions,\n",
    "                y=avg_ratio_values,\n",
    "                mode='lines',\n",
    "                line=dict(color='red', width=2, dash='dash'),\n",
    "                name=\"Ratio Trend\" if not legend_added['trend'] else \"\",\n",
    "                legendgroup=\"trend\",\n",
    "                showlegend=False, #(not legend_added['trend']),\n",
    "                hovertemplate=f\"<b>Trend Line</b><br><b>Metric:</b> {metric_label}<br><extra></extra>\"\n",
    "            ),\n",
    "            row=row+1, col=1, secondary_y=True\n",
    "        )\n",
    "        if not legend_added['trend']:\n",
    "            legend_added['trend'] = True\n",
    "# ---- Layout ----\n",
    "fig_combined.update_layout(\n",
    "    # title=dict(\n",
    "    #     text=\"<b>Numerical and Samples Variability and Varibility Ratio of Local Graph Metrics Across Thresholds</b>\",\n",
    "    #     font=dict(size=24, family=\"Arial Black\", color=\"black\"),\n",
    "    #     x=0.5\n",
    "    # ),\n",
    "    height=3600, width=2000,\n",
    "    margin=dict(l=140, r=70, t=100, b=100),\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        orientation=\"h\", yanchor=\"bottom\", y=-0.05,\n",
    "        xanchor=\"center\", x=0.5, font=dict(size=26)\n",
    "    ),\n",
    "    template='plotly_white',\n",
    "    font=dict(family=\"Arial\", size=20, color=\"black\")\n",
    ")\n",
    "# Increase subplot title font size\n",
    "for i, ann in enumerate(fig_combined.layout.annotations):\n",
    "    ann.font.size = 30  # subplot title font size\n",
    "# ---- Update axes ----\n",
    "for row in range(1, 5):\n",
    "    metric = metrics[row-1]\n",
    "    y_range = metric_ranges.get(metric, (0, 1))\n",
    "    if row == 3:\n",
    "        y_range=(0,0.05)\n",
    "    if row == 4:\n",
    "        y_range=(0,0.02)\n",
    "    # X-axis with thresholds\n",
    "    x_tick_positions = [i * 3 for i in range(len(thresholds))]\n",
    "    x_tick_labels = [f\"<b>T={t}</b>\" for t in thresholds]\n",
    "    \n",
    "    # X-axis - only show labels and title on the last row (row 4)\n",
    "    if row == 4:  # Last row\n",
    "        fig_combined.update_xaxes(\n",
    "            row=row, col=1,\n",
    "            tickvals=x_tick_positions,\n",
    "            ticktext=x_tick_labels,\n",
    "            tickfont=dict(size=20),\n",
    "            title=dict(text=\"<b>Threshold Values</b>\", font=dict(size=30)),\n",
    "            range=[-1, (len(thresholds)-1) * 3 + 1],\n",
    "            showticklabels=True\n",
    "        )\n",
    "    else:  # Other rows\n",
    "        fig_combined.update_xaxes(\n",
    "            row=row, col=1,\n",
    "            tickvals=x_tick_positions,\n",
    "            ticktext=[],  # No labels\n",
    "            tickfont=dict(size=26),\n",
    "            range=[-1, (len(thresholds)-1) * 3 + 1],\n",
    "            showticklabels=False  # Hide tick labels\n",
    "        )\n",
    "    \n",
    "    # Left y-axis (metrics)\n",
    "    fig_combined.update_yaxes(\n",
    "        row=row, col=1, secondary_y=False,\n",
    "        title=dict(text=\"<b>NV-PV</b>\", font=dict(size=28)),\n",
    "        range=[y_range[0], y_range[1]],\n",
    "        tickformat=\".2e\",\n",
    "        tickfont=dict(size=32)\n",
    "    )\n",
    "    \n",
    "    # Right y-axis (ratio)\n",
    "    fig_combined.update_yaxes(\n",
    "        row=row, col=1, secondary_y=True,\n",
    "        title=dict(text=\"<b>Averaged NPVR </b>\", font=dict(size=28)),\n",
    "        tickfont=dict(size=32),\n",
    "        showgrid=False\n",
    "    )\n",
    "\n",
    "# # Add explanation\n",
    "# fig_combined.add_annotation(\n",
    "#     text=\"<b>Blues represent Ïƒ_num | Orange represent Ïƒ_anat</b><br>\"\n",
    "#          \"<b>Red stars show average ratios | Red dashed line shows ratio trend across thresholds, and the average Variability Ratio across nods</b>\",\n",
    "#     xref=\"paper\", yref=\"paper\",\n",
    "#     x=0.5, y=-0.02,\n",
    "#     xanchor=\"center\", yanchor=\"top\",\n",
    "#     font=dict(size=20, color=\"gray\"),\n",
    "#     showarrow=False\n",
    "# )\n",
    "\n",
    "# ---- Show ----\n",
    "fig_combined.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "# ---- Setup ----\n",
    "metrics = ['smallworldness', 'avg_shortestPathLength']\n",
    "metric_labels = ['Small-worldness', 'Avg Shortest Path Length']\n",
    "thresholds = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Data for num and anat\n",
    "dataframes_num = [avrg05_Wconf_num, avrg1_Wconf_num, avrg2_Wconf_num, avrg3_Wconf_num, avrg4_Wconf_num, avrg5_Wconf_num]\n",
    "dataframes_anat = [avrg05_Wconf_anat, avrg1_Wconf_anat, avrg2_Wconf_anat, avrg3_Wconf_anat, avrg4_Wconf_anat, avrg5_Wconf_anat]\n",
    "dataframes_avrRatio = [ratio05G, ratio1G, ratio2G, ratio3G, ratio4G, ratio5G]\n",
    "\n",
    "# # Color schemes - getting darker with increasing threshold\n",
    "# def generate_color_palette(base_color, n_colors):\n",
    "#     if base_color == 'blue':\n",
    "#         colors = ['rgb(173, 216, 230)', 'rgb(135, 206, 250)', 'rgb(70, 130, 180)',\n",
    "#                   'rgb(25, 25, 112)', 'rgb(0, 0, 139)', 'rgb(0, 0, 80)']\n",
    "#     else:  # orange\n",
    "#         colors = ['rgb(255, 218, 185)', 'rgb(255, 165, 0)', 'rgb(255, 140, 0)',\n",
    "#                   'rgb(255, 69, 0)', 'rgb(210, 69, 0)', 'rgb(139, 69, 19)']\n",
    "#     return colors[:n_colors]\n",
    "\n",
    "# colors_num = generate_color_palette('blue', len(thresholds))\n",
    "# colors_anat = generate_color_palette('orange', len(thresholds))\n",
    "colors_num = px.colors.sequential.Blues[1:7]   # red spectrum\n",
    "colors_anat = px.colors.sequential.Oranges[1:7] # blue spectrum\n",
    "# Helper to extract values\n",
    "def extract_metric_value(df, metric):\n",
    "    try:\n",
    "        if isinstance(df, dict):\n",
    "            val = df.get(metric, np.nan)\n",
    "        else:\n",
    "            if hasattr(df, 'loc'):\n",
    "                if metric in df.index:\n",
    "                    val = df.loc[metric]\n",
    "                elif metric in df.columns:\n",
    "                    val = df[metric]\n",
    "                else:\n",
    "                    matching_cols = [col for col in df.columns if metric in str(col)]\n",
    "                    val = df[matching_cols[0]] if matching_cols else np.nan\n",
    "            else:\n",
    "                val = df\n",
    "\n",
    "        if hasattr(val, 'iloc'):\n",
    "            val = val.iloc[0] if len(val) == 1 else val.mean()\n",
    "        elif hasattr(val, 'values'):\n",
    "            val = val.values[0] if len(val.values) == 1 else np.mean(val.values)\n",
    "\n",
    "        return float(val) if not pd.isna(val) else np.nan\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {metric}: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=len(metrics), cols=1,\n",
    "    subplot_titles=[f\"<b>{label}</b>\" for label in metric_labels],\n",
    "    \n",
    "    specs=[[{\"secondary_y\": True}] for _ in range(len(metrics))],\n",
    "    vertical_spacing=0.15\n",
    ")\n",
    "\n",
    "offset = 0.005  # <-- shift for side-by-side circles\n",
    "\n",
    "# Process each metric\n",
    "for metric_idx, metric in enumerate(metrics):\n",
    "    row = metric_idx + 1\n",
    "    \n",
    "    num_values, anat_values, ratio_values = [], [], []\n",
    "    \n",
    "    for thresh_idx, threshold in enumerate(thresholds):\n",
    "        num_val = extract_metric_value(dataframes_num[thresh_idx], metric)\n",
    "        anat_val = extract_metric_value(dataframes_anat[thresh_idx], metric)\n",
    "        ratio_val = extract_metric_value(dataframes_avrRatio[thresh_idx], metric)\n",
    "        \n",
    "        num_values.append(num_val if not pd.isna(num_val) else 0)\n",
    "        anat_values.append(anat_val if not pd.isna(anat_val) else 0)\n",
    "        ratio_values.append(ratio_val if not pd.isna(ratio_val) else 0)\n",
    "    \n",
    "    # side-by-side x\n",
    "    num_x = [t - offset for t in thresholds]\n",
    "    anat_x = [t + offset for t in thresholds]\n",
    "\n",
    "    # Numerical scatter\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=num_x, y=num_values,\n",
    "            mode='markers+text',\n",
    "            name='(Numerical Variability (NV), Ïƒ_num)',\n",
    "            marker=dict(color='blue', size=10, line=dict(color='black', width=1)),\n",
    "            legendgroup='numerical',\n",
    "                    legendrank=1,\n",
    "\n",
    "            showlegend=(metric_idx == 1)\n",
    "        ),\n",
    "        row=row, col=1\n",
    "    )\n",
    "\n",
    "    # Anatomical scatter\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=anat_x, y=anat_values,\n",
    "            mode='markers+text',\n",
    "            name='(Population Variability (PV), Ïƒ_pop)',\n",
    "            marker=dict(color='orange', size=10,  line=dict(color='black', width=1)),\n",
    "            legendgroup='anatomical',\n",
    "                    legendrank=2,\n",
    "\n",
    "            showlegend=(metric_idx == 1)\n",
    "        ),\n",
    "        row=row, col=1\n",
    "    )\n",
    "\n",
    "    # Ratio line with stars\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=thresholds,\n",
    "            y=ratio_values,\n",
    "            mode='lines+markers',\n",
    "            name='NPVR',\n",
    "            line=dict(color='red', width=3, dash='dash'),\n",
    "            marker=dict(symbol='star', size=12, color='red', line=dict(color='black', width=1)),\n",
    "            legendgroup='ratio',\n",
    "                    legendrank=3,\n",
    "\n",
    "            showlegend=(metric_idx == 0),\n",
    "            yaxis=f'y{2*metric_idx+2}' if metric_idx > 0 else 'y2'\n",
    "        ),\n",
    "        row=row, col=1, secondary_y=True\n",
    "    )\n",
    "# Increase font size of subplot titles\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font'] = dict(size=30, family='Arial Black', color='black')\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    # title={\n",
    "    #     'text': '<b>Numerical and Samples Variability and Varibility Ratio of Global Graph Metrics Across Thresholds</b>',\n",
    "    #     'x': 0.5, 'xanchor': 'center', 'yanchor': 'bottom'\n",
    "    # },\n",
    "    font=dict(size=22, family=\"Arial Black\", color=\"black\"),\n",
    "    height=400 * len(metrics),\n",
    "    width=1800,\n",
    "    template='plotly_white',\n",
    "    showlegend=True,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\", y=-0.17,\n",
    "        xanchor=\"center\", x=0.5, font=dict(size=26)\n",
    "    ),\n",
    "    barmode='overlay'\n",
    ")\n",
    "\n",
    "# Axes\n",
    "for i in range(len(metrics)):\n",
    "    row = i + 1\n",
    "    fig.update_yaxes(title_text='<b>NV-PV</b>', row=row, col=1,  tickfont=dict(size=28))\n",
    "    fig.update_yaxes(title_text=\"<b>NPVR</b>\", row=row, col=1, secondary_y=True, tickfont=dict(size=28),showgrid=False)\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"<b>Threshold</b>\" if i == len(metrics)-1 else \"\",\n",
    "        tickvals=thresholds, ticktext=[f\"<b>T={str(t)}</b>\" for t in thresholds],\n",
    "        tickfont=dict(size=20),\n",
    "        \n",
    "        row=row, col=1\n",
    "    )\n",
    "\n",
    "fig.show()\n",
    "# Print summary\n",
    "print(\"Data Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for metric_idx, metric in enumerate(metrics):\n",
    "    print(f\"\\n{metric_labels[metric_idx]}:\")\n",
    "    print(\"-\" * 30)\n",
    "    for thresh_idx, threshold in enumerate(thresholds):\n",
    "        num_val = extract_metric_value(dataframes_num[thresh_idx], metric)\n",
    "        anat_val = extract_metric_value(dataframes_anat[thresh_idx], metric)\n",
    "        ratio_val = extract_metric_value(dataframes_avrRatio[thresh_idx], metric)\n",
    "        print(f\"Threshold {threshold}: Num={num_val:.6f}, Anat={anat_val:.6f}, Ratio={ratio_val:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import image, plotting, datasets\n",
    "import tempfile\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 16,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'legend.fontsize': 14,\n",
    "    'figure.titlesize': 20\n",
    "})\n",
    "# --------------------------\n",
    "# Load Schaefer Atlas (100 regions)\n",
    "# --------------------------\n",
    "atlas = datasets.fetch_atlas_schaefer_2018(n_rois=100, yeo_networks=7)\n",
    "atlas_filename = atlas.maps\n",
    "atlas_labels = atlas.labels[1:]\n",
    "\n",
    "# --------------------------\n",
    "# Your ratio computations (already available)\n",
    "# --------------------------\n",
    "ratio1 = cmput_ratio(avrg1_Wconf_num, avrg1_Wconf_anat, 182)\n",
    "ratio2 = cmput_ratio(avrg2_Wconf_num, avrg2_Wconf_anat, 179)\n",
    "ratio3 = cmput_ratio(avrg3_Wconf_num, avrg3_Wconf_anat, 176)\n",
    "ratio4 = cmput_ratio(avrg4_Wconf_num, avrg4_Wconf_anat, 170)\n",
    "ratio5 = cmput_ratio(avrg5_Wconf_num, avrg5_Wconf_anat, 147)\n",
    "ratio05 = cmput_ratio(avrg05_Wconf_num, avrg05_Wconf_anat, 182)\n",
    "\n",
    "ratio_map = {\n",
    "    \"05\": ratio05,\n",
    "    \"1\": ratio1,\n",
    "    \"2\": ratio2,\n",
    "    \"3\": ratio3,\n",
    "    \"4\": ratio4,\n",
    "    \"5\": ratio5\n",
    "}\n",
    "\n",
    "thresholds =[0.05, 0.1, 0.2, 0.3, 0.4, 0.5] #[0.1,0.4] # [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "threshold_labels = [\"0.05\", \"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\"] #[\"0.1\",\"0.4\"] #[\"0.05\", \"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\"]\n",
    "metrics = ['betweeness', 'clusteringcoef', 'eigen', 'degree']\n",
    "metric_labels = [\"Betweenness Centrality\", \"Clustering Coefficient\", \"Eigenvector Centrality\", \"Degree Centrality\"]\n",
    "\n",
    "# --------------------------\n",
    "# Compute SD table\n",
    "# --------------------------\n",
    "sd_table = pd.DataFrame(columns=['Threshold', 'Metric', 'Std'])\n",
    "\n",
    "for thr in thresholds:\n",
    "    thre = str(thr).split(\".\")[1] if \".\" in str(thr) else str(int(thr))\n",
    "    rPD = ratio_map[thre]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        vals = rPD[[c for c in rPD.columns if metric in c]].iloc[0, 0]\n",
    "        std_val = pd.Series(vals).std()\n",
    "        sd_table.loc[len(sd_table)] = [thr, metric_labels[i], std_val]\n",
    "\n",
    "# --------------------------\n",
    "# Function: map ratios to brain\n",
    "# --------------------------\n",
    "def map_ratio_to_brain(ratio_values, atlas_filename):\n",
    "    atlas_img = image.load_img(atlas_filename)\n",
    "    atlas_data = atlas_img.get_fdata()\n",
    "    ratio_values = np.nan_to_num(ratio_values)\n",
    "    ratio_img_data = np.zeros_like(atlas_data)\n",
    "    for i, val in enumerate(ratio_values, start=1):\n",
    "        ratio_img_data[atlas_data == i] = val\n",
    "    return image.new_img_like(atlas_img, ratio_img_data)\n",
    "\n",
    "# --------------------------\n",
    "# Temporary directory for snapshots\n",
    "# --------------------------\n",
    "tmpdir = tempfile.mkdtemp()\n",
    "\n",
    "# --------------------------\n",
    "# Compute global vmin/vmax for consistent colorbar\n",
    "# --------------------------\n",
    "# all_vals = []\n",
    "# for thr in thresholds:\n",
    "#     thre = str(thr).split(\".\")[1] if \".\" in str(thr) else str(int(thr))\n",
    "#     rPD = ratio_map[thre]\n",
    "#     for metric in metrics:\n",
    "#         vals = rPD[[c for c in rPD.columns if metric in c]].iloc[0, 0]\n",
    "#         all_vals.extend(vals)\n",
    "# vmin, vmax = np.nanmin(all_vals), np.nanmax(all_vals)\n",
    "# --------------------------\n",
    "# Generate brain map images and store paths\n",
    "# --------------------------\n",
    "image_paths = {metric: [] for metric in metrics}\n",
    "\n",
    "for metric in metrics:\n",
    "    for thr in thresholds:\n",
    "        thre = str(thr).split(\".\")[1] if \".\" in str(thr) else str(int(thr))\n",
    "        rPD = ratio_map[thre]\n",
    "        ratio_values = rPD[[c for c in rPD.columns if metric in c]].iloc[0, 0]\n",
    "        # ratio_img = map_ratio_to_brain(ratio_values, atlas_filename)\n",
    "        vmin, vmax = 0,1 #np.nanmin(ratio_values), np.nanmax(ratio_values) #compute vmin,vmax for each metric separately or select  global vmin/vmax\n",
    "        # Normalize (0â€“1 range)\n",
    "        norm_vals = (ratio_values - np.min(ratio_values)) / (\n",
    "            np.max(ratio_values) - np.min(ratio_values)\n",
    "        )\n",
    "        ratio_img = map_ratio_to_brain(norm_vals, atlas_filename)\n",
    "        out_path = os.path.join(tmpdir, f\"{metric}_thr{thre}.png\")\n",
    "        plotting.plot_stat_map(\n",
    "            ratio_img,\n",
    "            display_mode='ortho', #pick view\n",
    "            cut_coords=[2,0,0],\n",
    "            cmap='Reds',\n",
    "            colorbar=True,\n",
    "            black_bg=False,\n",
    "            dim=0,\n",
    "            threshold=0 ,#vmin - 10**-6,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            output_file=out_path\n",
    "        )\n",
    "        image_paths[metric].append(out_path)\n",
    "        \n",
    "\n",
    "# --------------------------\n",
    "# Assemble figure grid (metrics as rows, thresholds as columns)\n",
    "# --------------------------\n",
    "n_rows = len(metrics)\n",
    "n_cols = len(thresholds)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 10))\n",
    "plt.subplots_adjust(wspace=0.01, hspace=0.05, left=0.08, right=0.90, bottom=0.08, top=0.92)\n",
    "\n",
    "for i, metric_label in enumerate(metric_labels):\n",
    "    metric = metrics[i]\n",
    "    for j, thr_label in enumerate(threshold_labels):\n",
    "        img = plt.imread(image_paths[metric][j])\n",
    "        axes[i, j].imshow(img)\n",
    "        axes[i, j].axis(\"off\")\n",
    "\n",
    "        # Column titles: thresholds\n",
    "        if i == 0:\n",
    "            axes[i, j].set_title(f\"Threshold = {thr_label}\", fontsize=11, pad=8)\n",
    "\n",
    "        # Row labels: metrics\n",
    "        if j == 0:\n",
    "            axes[i, j].text(-0.15, 0.5, metric_label,\n",
    "                            transform=axes[i, j].transAxes,\n",
    "                            fontsize=11, va='center', ha='right',\n",
    "                            weight='bold')\n",
    "\n",
    "        # SD annotation\n",
    "        sd_value = sd_table.loc[\n",
    "            (sd_table[\"Threshold\"] == float(thr_label)) &\n",
    "            (sd_table[\"Metric\"] == metric_label),\n",
    "            \"Std\"\n",
    "        ].values[0]\n",
    "        axes[i, j].text(0.5, 0.95, f\"SD = {sd_value:.3f}\",\n",
    "                        transform=axes[i, j].transAxes,\n",
    "                        ha='center', va='top', fontsize=10, color='black')\n",
    "\n",
    "# --------------------------\n",
    "# Shared colorbar\n",
    "# --------------------------\n",
    "# sm = plt.cm.ScalarMappable(cmap='jet', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "# cbar_ax = fig.add_axes([0.93, 0.15, 0.02, 0.7])\n",
    "# cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "# cbar.set_label(\"Variability Ratio\", rotation=270, labelpad=15)\n",
    "\n",
    "# --------------------------\n",
    "# Figure title\n",
    "# --------------------------\n",
    "fig.suptitle(\"Variability Ratio of No Confound Pipeline Across Regions for differnt Thresholds (Rows = Metrics, Columns = Thresholds)\", fontsize=14, y=0.99)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import image, plotting, datasets\n",
    "import tempfile\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 16,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'legend.fontsize': 14,\n",
    "    'figure.titlesize': 20\n",
    "})\n",
    "# --------------------------\n",
    "# Load Schaefer Atlas (100 regions)\n",
    "# --------------------------\n",
    "atlas = datasets.fetch_atlas_schaefer_2018(n_rois=100, yeo_networks=7)\n",
    "atlas_filename = atlas.maps\n",
    "atlas_labels = atlas.labels[1:]\n",
    "\n",
    "# --------------------------\n",
    "# Your ratio computations (already available)\n",
    "# --------------------------\n",
    "ratio1 = cmput_ratio(avrg1_Wconf_num, avrg1_Wconf_anat, 182)\n",
    "ratio2 = cmput_ratio(avrg2_Wconf_num, avrg2_Wconf_anat, 179)\n",
    "ratio3 = cmput_ratio(avrg3_Wconf_num, avrg3_Wconf_anat, 176)\n",
    "ratio4 = cmput_ratio(avrg4_Wconf_num, avrg4_Wconf_anat, 170)\n",
    "ratio5 = cmput_ratio(avrg5_Wconf_num, avrg5_Wconf_anat, 147)\n",
    "ratio05 = cmput_ratio(avrg05_Wconf_num, avrg05_Wconf_anat, 182)\n",
    "\n",
    "ratio_map = {\n",
    "    \"05\": ratio05,\n",
    " \n",
    "    \"5\": ratio5\n",
    "}\n",
    "\n",
    "thresholds =[0.05, 0.5] #[0.1,0.4] # [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "threshold_labels = [\"0.05\",  \"0.5\"] #[\"0.1\",\"0.4\"] #[\"0.05\", \"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\"]\n",
    "metrics = ['betweeness', 'clusteringcoef', 'eigen', 'degree']\n",
    "metric_labels = [\"Betweenness Centrality\", \"Clustering Coefficient\", \"Eigenvector Centrality\", \"Degree Centrality\"]\n",
    "\n",
    "# --------------------------\n",
    "# Compute SD table\n",
    "# --------------------------\n",
    "sd_table = pd.DataFrame(columns=['Threshold', 'Metric', 'Std'])\n",
    "\n",
    "for thr in thresholds:\n",
    "    thre = str(thr).split(\".\")[1] if \".\" in str(thr) else str(int(thr))\n",
    "    rPD = ratio_map[thre]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        vals = rPD[[c for c in rPD.columns if metric in c]].iloc[0, 0]\n",
    "        std_val = pd.Series(vals).std()\n",
    "        sd_table.loc[len(sd_table)] = [thr, metric_labels[i], std_val]\n",
    "\n",
    "# --------------------------\n",
    "# Function: map ratios to brain\n",
    "# --------------------------\n",
    "def map_ratio_to_brain(ratio_values, atlas_filename):\n",
    "    atlas_img = image.load_img(atlas_filename)\n",
    "    atlas_data = atlas_img.get_fdata()\n",
    "    ratio_values = np.nan_to_num(ratio_values)\n",
    "    ratio_img_data = np.zeros_like(atlas_data)\n",
    "    for i, val in enumerate(ratio_values, start=1):\n",
    "        ratio_img_data[atlas_data == i] = val\n",
    "    return image.new_img_like(atlas_img, ratio_img_data)\n",
    "\n",
    "# --------------------------\n",
    "# Temporary directory for snapshots\n",
    "# --------------------------\n",
    "tmpdir = tempfile.mkdtemp()\n",
    "\n",
    "# --------------------------\n",
    "# Compute global vmin/vmax for consistent colorbar\n",
    "# --------------------------\n",
    "# all_vals = []\n",
    "# for thr in thresholds:\n",
    "#     thre = str(thr).split(\".\")[1] if \".\" in str(thr) else str(int(thr))\n",
    "#     rPD = ratio_map[thre]\n",
    "#     for metric in metrics:\n",
    "#         vals = rPD[[c for c in rPD.columns if metric in c]].iloc[0, 0]\n",
    "#         all_vals.extend(vals)\n",
    "# vmin, vmax = np.nanmin(all_vals), np.nanmax(all_vals)\n",
    "# --------------------------\n",
    "# Generate brain map images and store paths\n",
    "# --------------------------\n",
    "image_paths = {metric: [] for metric in metrics}\n",
    "\n",
    "for metric in metrics:\n",
    "    for thr in thresholds:\n",
    "        thre = str(thr).split(\".\")[1] if \".\" in str(thr) else str(int(thr))\n",
    "        rPD = ratio_map[thre]\n",
    "        ratio_values = rPD[[c for c in rPD.columns if metric in c]].iloc[0, 0]\n",
    "        ratio_img = map_ratio_to_brain(ratio_values, atlas_filename)\n",
    "        vmin, vmax = 0,1 #np.nanmin(ratio_values), np.nanmax(ratio_values) #compute vmin,vmax for each metric separately or select  global vmin/vmax\n",
    "        # Normalize (0â€“1 range)\n",
    "        out_path = os.path.join(tmpdir, f\"{metric}_thr{thre}.png\")\n",
    "        plotting.plot_stat_map(\n",
    "            ratio_img,\n",
    "            display_mode='ortho', #pick view\n",
    "            cut_coords=[2,0,0],\n",
    "            cmap='Reds',\n",
    "            colorbar=True,\n",
    "            black_bg=False,\n",
    "            dim=0,\n",
    "            threshold=0 ,#vmin - 10**-6,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            output_file=out_path\n",
    "        )\n",
    "        image_paths[metric].append(out_path)\n",
    "        \n",
    "\n",
    "# --------------------------\n",
    "# Assemble figure grid (metrics as rows, thresholds as columns)\n",
    "# --------------------------\n",
    "n_rows = len(metrics)\n",
    "n_cols = len(thresholds)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 10))\n",
    "plt.subplots_adjust(wspace=0.01, hspace=0.05, left=0.08, right=0.90, bottom=0.08, top=0.92)\n",
    "\n",
    "for i, metric_label in enumerate(metric_labels):\n",
    "    metric = metrics[i]\n",
    "    for j, thr_label in enumerate(threshold_labels):\n",
    "        img = plt.imread(image_paths[metric][j])\n",
    "        axes[i, j].imshow(img)\n",
    "        axes[i, j].axis(\"off\")\n",
    "\n",
    "        # Column titles: thresholds\n",
    "        if i == 0:\n",
    "            axes[i, j].set_title(f\"Threshold = {thr_label}\", fontsize=11, pad=8)\n",
    "\n",
    "        # Row labels: metrics\n",
    "        if j == 0:\n",
    "            axes[i, j].text(-0.15, 0.5, metric_label,\n",
    "                            transform=axes[i, j].transAxes,\n",
    "                            fontsize=11, va='center', ha='right',\n",
    "                            weight='bold')\n",
    "\n",
    "        # SD annotation\n",
    "        sd_value = sd_table.loc[\n",
    "            (sd_table[\"Threshold\"] == float(thr_label)) &\n",
    "            (sd_table[\"Metric\"] == metric_label),\n",
    "            \"Std\"\n",
    "        ].values[0]\n",
    "        axes[i, j].text(0.5, 0.95, f\"SD = {sd_value:.3f}\",\n",
    "                        transform=axes[i, j].transAxes,\n",
    "                        ha='center', va='top', fontsize=10, color='black')\n",
    "\n",
    "# --------------------------\n",
    "# Shared colorbar\n",
    "# --------------------------\n",
    "# sm = plt.cm.ScalarMappable(cmap='jet', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "# cbar_ax = fig.add_axes([0.93, 0.15, 0.02, 0.7])\n",
    "# cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "# cbar.set_label(\"Variability Ratio\", rotation=270, labelpad=15)\n",
    "\n",
    "# --------------------------\n",
    "# Figure title\n",
    "# --------------------------\n",
    "fig.suptitle(\"Variability Ratio of No Confound Pipeline Across Regions for differnt Thresholds (Rows = Metrics, Columns = Thresholds)\", fontsize=14, y=0.99)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
