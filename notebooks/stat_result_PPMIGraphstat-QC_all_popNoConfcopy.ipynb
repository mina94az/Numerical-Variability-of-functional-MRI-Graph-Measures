{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U kaleido\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import significantdigits as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove failed subjects\n",
    "# subjects_to_remove =  ['sub-#', 'sub-##', 'sub-###']\n",
    "# hcsubjects_to_remove=['sub-####', 'sub-#####']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data function for each threshold and removing subject failed in preprocessing\n",
    "\n",
    "def Read_data(threshold):\n",
    "    #ReadWconf data\n",
    "    df=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableWConf_batch_1.pkl')\n",
    "    df1=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableWConf_batch_2.pkl')\n",
    "    df2=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableWConf_batch_3.pkl')\n",
    "    df3=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableWConf_batch_4.pkl')\n",
    "    df4=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableWConf_batch_5.pkl')\n",
    "\n",
    "    all_dfs = [df, df1, df2, df3, df4]\n",
    "    Results_tableWConf = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "    ################################################################################################\n",
    "\n",
    "    #Read NoConf data\n",
    "    dfn=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableNoConf_batch_1.pkl')\n",
    "    dfn1=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableNoConf_batch_2.pkl')\n",
    "    dfn2=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableNoConf_batch_3.pkl')\n",
    "    dfn3=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableNoConf_batch_4.pkl')\n",
    "    dfn4=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableNoConf_batch_5.pkl')\n",
    "\n",
    "    all_dfs = [dfn, dfn1, dfn2, dfn3, dfn4]\n",
    "    Results_tableNoConf = pd.concat(all_dfs, ignore_index=True)  \n",
    "    #Remove failed subjects\n",
    "\n",
    " \n",
    "    dfW = Results_tableWConf[~Results_tableWConf['subject'].isin(subjects_to_remove)]\n",
    "    dfN= Results_tableNoConf[~Results_tableNoConf['subject'].isin(subjects_to_remove)]\n",
    "    \n",
    "    ###################################################################################################\n",
    "\n",
    "    #Read healthy control\n",
    "    dfWhc=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableWConf_batch_hc.pkl')\n",
    "    dfNhc=pd.read_pickle(f'/home/ubuntu/Desktop/Thesis/overlap/allbatches/table_correther{threshold}/Result_tableNoConf_batch_hc.pkl')\n",
    "    dfWhc = dfWhc[~dfWhc['subject'].isin(hcsubjects_to_remove)]\n",
    "    dfNhc= dfNhc[~dfNhc['subject'].isin(hcsubjects_to_remove)]\n",
    "\n",
    "    return dfW,dfN,dfWhc,dfNhc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW,dfN,dfWhc,dfNhc=Read_data(0.5)\n",
    "alldfss_p=[dfN,dfNhc]\n",
    "all_pop5=pd.concat(alldfss_p,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW,dfN,dfWhc,dfNhc=Read_data(0.4)\n",
    "alldfss_g=[dfN,dfNhc]\n",
    "all_pop4=pd.concat(alldfss_g,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW,dfN,dfWhc,dfNhc=Read_data(0.3)\n",
    "alldfss_s=[dfN,dfNhc]\n",
    "all_pop3=pd.concat(alldfss_s,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW,dfN,dfWhc,dfNhc=Read_data(0.2)\n",
    "alldfss_d=[dfN,dfNhc]\n",
    "all_pop2=pd.concat(alldfss_d,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW,dfN,dfWhc,dfNhc=Read_data(0.1)\n",
    "alldfss_o=[dfN,dfNhc]\n",
    "all_pop1=pd.concat(alldfss_o,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW,dfN,dfWhc,dfNhc=Read_data(0.05)\n",
    "alldfss_ps=[dfN,dfNhc]\n",
    "all_pop05=pd.concat(alldfss_ps,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the variance across MCA repetitions for each subject, session, and acquisition\n",
    "\n",
    "def makestdevNum_metric(df):\n",
    "    \"\"\"\n",
    "    Calculate standard deviation across MCA repetitions for each subject, session, and acquisition.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing network metrics for different subjects, sessions, acquisitions and repetitions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with standard deviations of metrics for each subject, session, and acquisition\n",
    "    \"\"\"\n",
    "    # Define columns for metrics\n",
    "    metric_columns = [\n",
    "        'degree_(numericalVar)', 'betweeness_(numericalVar)', \n",
    "          'eigenvec_(numericalVar)',  'clusteringcoef_(numericalVar)',\n",
    "        'smallworldness(numericalVar)', 'avg_shortestPathLength(numericalVar)'\n",
    "    ]\n",
    "    \n",
    "    # Define info columns\n",
    "    info_columns = ['subject', 'session', 'acquisition']\n",
    "    \n",
    "    # Pre-filter to exclude unwanted acquisitions for efficiency\n",
    "    df_filtered = df[~df['acquisition'].isin(['acq-RLsplit1', 'acq-LRsplit1'])]\n",
    "    \n",
    "    # Create empty list to collect results\n",
    "    results = []\n",
    "    \n",
    "    # Group by subject, session, acquisition to avoid nested loops\n",
    "    for (subj, session, acquisition), group in df_filtered.groupby(['subject', 'session', 'acquisition']):\n",
    "        # Collections for each metric across repetitions\n",
    "        all_metrics = {\n",
    "            'degree': [], 'betweeness': [], 'eigen': [], \n",
    "            'clustering': [], 'smallworld': [], 'shortestpath': []\n",
    "        }\n",
    "        \n",
    "        # Collect metrics for all repetitions\n",
    "        for _, row in group.iterrows():\n",
    "            rep_num = row.get('repetition')\n",
    "            if not isinstance(rep_num, str) or not rep_num.startswith('rep-'):\n",
    "                continue\n",
    "                \n",
    "            # Only process if we have the necessary data\n",
    "            if (isinstance(row.get('degree_centralities'), dict) and \n",
    "                isinstance(row.get('betweenness_centralities'), dict) and\n",
    "                isinstance(row.get('eigenvector_centralities'), dict) and\n",
    "                isinstance(row.get('clustering_coefficients'), dict)):\n",
    "                \n",
    "                # Extract metrics\n",
    "                all_metrics['degree'].append(list(row['degree_centralities'].values()))\n",
    "                all_metrics['betweeness'].append(list(row['betweenness_centralities'].values()))\n",
    "                all_metrics['eigen'].append(list(row['eigenvector_centralities'].values()))\n",
    "                all_metrics['clustering'].append(list(row['clustering_coefficients'].values()))\n",
    "                \n",
    "                # Add scalar metrics if they exist\n",
    "                if 'small_worldness' in row:\n",
    "                    all_metrics['smallworld'].append(row['small_worldness'])\n",
    "                if 'avg_shortest_path_length' in row:\n",
    "                    all_metrics['shortestpath'].append(row['avg_shortest_path_length'])\n",
    "        \n",
    "        # Only calculate stats if we have data\n",
    "        if not all_metrics['degree']:\n",
    "            continue\n",
    "            \n",
    "        # Calculate standard deviations\n",
    "        result_row = {\n",
    "            'subject': subj,\n",
    "            'session': session,\n",
    "            'acquisition': acquisition,\n",
    "            'degree_(numericalVar)': [np.var(all_metrics['degree'], axis=0)],\n",
    "            'betweeness_(numericalVar)': [np.var(all_metrics['betweeness'], axis=0)],\n",
    "            'eigenvec_(numericalVar)': [np.var(all_metrics['eigen'], axis=0)],\n",
    "            'clusteringcoef_(numericalVar)': [np.var(all_metrics['clustering'], axis=0)]\n",
    "        }\n",
    "        \n",
    "        # Add scalar metrics if available\n",
    "        if all_metrics['smallworld']:\n",
    "            result_row['smallworldness(numericalVar)'] = np.var(all_metrics['smallworld'], axis=0)\n",
    "        if all_metrics['shortestpath']:\n",
    "            result_row['avg_shortestPathLength(numericalVar)'] = np.var(all_metrics['shortestpath'], axis=0)\n",
    "            \n",
    "        results.append(result_row)\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    avrofStd_WithinSubject = pd.DataFrame(results)\n",
    "    \n",
    "    # Ensure proper data types for object columns\n",
    "    for col in metric_columns:\n",
    "        if col in avrofStd_WithinSubject.columns:\n",
    "            avrofStd_WithinSubject[col] = avrofStd_WithinSubject[col].astype('object')\n",
    "    \n",
    "    return avrofStd_WithinSubject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the variance across subjects for each repetition, session, and acquisition\n",
    "\n",
    "def makestedvAnat_metric(df):\n",
    "    \"\"\"\n",
    "    Calculate standard deviation of network metrics across subjects for each repetition, session, and acquisition.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing network metrics for different subjects, sessions, acquisitions and repetitions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with standard deviations of metrics for each repetition, session, and acquisition\n",
    "    \"\"\"\n",
    "    # Define columns for metrics\n",
    "    metric_columns = [\n",
    "        'degree_(AnatomicalVar)', 'betweeness_(AnatomicalVar)', \n",
    "         'eigenvec_(AnatomicalVar)', 'clusteringcoef_(AnatomicalVar)', \n",
    "        'smallworldness(AnatomicalVar)', 'avg_shortestPathLength(AnatomicalVar)'\n",
    "    ]\n",
    "    \n",
    "    # Info columns\n",
    "    info_columns = ['iteration', 'session', 'acquisition']\n",
    "    \n",
    "    # Pre-filter to exclude unwanted acquisitions for efficiency\n",
    "    df_filtered = df[~df['acquisition'].isin(['acq-RLsplit1', 'acq-LRsplit1'])]\n",
    "    \n",
    "    # Create empty list to collect results\n",
    "    results = []\n",
    "    \n",
    "    # First loop through repetitions (since we want to group by repetition)\n",
    "    for rep in range(1, 11):\n",
    "        rep_str = f'rep-{rep}'\n",
    "        # Filter for current repetition\n",
    "        rep_df = df_filtered[df_filtered['repetition'] == rep_str]\n",
    "        \n",
    "        if rep_df.empty:\n",
    "            continue\n",
    "            \n",
    "        # Group by session and acquisition\n",
    "        for (session, acquisition), group in rep_df.groupby(['session', 'acquisition']):\n",
    "            # Collections for metrics across subjects\n",
    "            all_metrics = {\n",
    "                'degree': [], 'betweeness': [], 'eigen': [], \n",
    "                'clustering': [], 'smallworld': [], 'shortestpath': []\n",
    "            }\n",
    "            \n",
    "            # Loop through subjects in this group\n",
    "            for subject, subject_data in group.groupby('subject'):\n",
    "                if subject_data.empty:\n",
    "                    continue\n",
    "                    \n",
    "                # Get the first (and should be only) row for this subject in this repetition/session/acquisition\n",
    "                row = subject_data.iloc[0]\n",
    "                \n",
    "                # Check if required data exists\n",
    "                if (isinstance(row.get('degree_centralities'), dict) and \n",
    "                    isinstance(row.get('betweenness_centralities'), dict) and\n",
    "                    isinstance(row.get('eigenvector_centralities'), dict) and\n",
    "                    isinstance(row.get('clustering_coefficients'), dict)):\n",
    "                    \n",
    "                    # Extract metrics\n",
    "                    all_metrics['degree'].append(list(row['degree_centralities'].values()))\n",
    "                    all_metrics['betweeness'].append(list(row['betweenness_centralities'].values()))\n",
    "                    all_metrics['eigen'].append(list(row['eigenvector_centralities'].values()))\n",
    "                    all_metrics['clustering'].append(list(row['clustering_coefficients'].values()))\n",
    "                    \n",
    "                    # Add scalar metrics if they exist\n",
    "                    if 'small_worldness' in row:\n",
    "                        all_metrics['smallworld'].append(row['small_worldness'])\n",
    "                    if 'avg_shortest_path_length' in row:\n",
    "                        all_metrics['shortestpath'].append(row['avg_shortest_path_length'])\n",
    "            # if((session=='1') & (acquisition=='acq-RL')):\n",
    "            #     print(len(all_metrics['degree']))\n",
    "            # Only calculate stats if we have enough data (at least 2 subjects)\n",
    "            if len(all_metrics['degree']) < 2:\n",
    "                continue\n",
    "            # if ((session=='1') & (acquisition=='acq-RL')):\n",
    "            #     print(len(all_metrics['degree']))\n",
    "            # Calculate standard deviations\n",
    "            result_row = {\n",
    "                'iteration': f'iter_{rep}',\n",
    "                'session': session,\n",
    "                'acquisition': acquisition,\n",
    "                'degree_(AnatomicalVar)': [np.var(all_metrics['degree'], axis=0)],\n",
    "                'betweeness_(AnatomicalVar)': [np.var(all_metrics['betweeness'], axis=0)],\n",
    "                # Note: Fixed a bug in the original code - eigenvec and clusteringcoef were swapped\n",
    "                'eigenvec_(AnatomicalVar)': [np.var(all_metrics['eigen'], axis=0)],\n",
    "                'clusteringcoef_(AnatomicalVar)': [np.var(all_metrics['clustering'], axis=0)]\n",
    "            }\n",
    "            \n",
    "            # Add scalar metrics if available\n",
    "            if all_metrics['smallworld']:\n",
    "                result_row['smallworldness(AnatomicalVar)'] = np.var(all_metrics['smallworld'], axis=0)\n",
    "            if all_metrics['shortestpath']:\n",
    "                result_row['avg_shortestPathLength(AnatomicalVar)'] = np.var(all_metrics['shortestpath'], axis=0)\n",
    "                \n",
    "            results.append(result_row)\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    avrofStd_BetweenSubject = pd.DataFrame(results)\n",
    "    \n",
    "    # Ensure proper data types for object columns\n",
    "    for col in metric_columns:\n",
    "        if col in avrofStd_BetweenSubject.columns:\n",
    "            avrofStd_BetweenSubject[col] = avrofStd_BetweenSubject[col].astype('object')\n",
    "    \n",
    "    return avrofStd_BetweenSubject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df5_Wconf_num=makestdevNum_metric(all_pop5)\n",
    "df5_Wconf_anat=makestedvAnat_metric(all_pop5)\n",
    "\n",
    "#######################################\n",
    "df4_Wconf_num=makestdevNum_metric(all_pop4)\n",
    "df4_Wconf_anat=makestedvAnat_metric(all_pop4)\n",
    "\n",
    "################################################\n",
    "df3_Wconf_num=makestdevNum_metric(all_pop3)\n",
    "df3_Wconf_anat=makestedvAnat_metric(all_pop3)\n",
    "\n",
    "##########################################################\n",
    "df2_Wconf_num=makestdevNum_metric(all_pop2)\n",
    "df2_Wconf_anat=makestedvAnat_metric(all_pop2)\n",
    "\n",
    "#########################################################\n",
    "\n",
    "df1_Wconf_num=makestdevNum_metric(all_pop1)\n",
    "df1_Wconf_anat=makestedvAnat_metric(all_pop1)\n",
    "\n",
    "#######################################################################################\n",
    "\n",
    "df05_Wconf_num=makestdevNum_metric(all_pop05)\n",
    "df05_Wconf_anat=makestedvAnat_metric(all_pop05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the root square of the average of variances across MCA runs and subjects\n",
    "\n",
    "def compute_average_anat(df_Noconf_anat, df_Wconf_anat):\n",
    "    \"\"\"\n",
    "    Compute average network metrics for 'no confound' and 'with confound' conditions across 10 MCA runs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_Noconf_anat : pandas.DataFrame\n",
    "        DataFrame containing anatomical network metrics for 'no confound' condition\n",
    "    df_Wconf_anat : pandas.DataFrame\n",
    "        DataFrame containing anatomical network metrics for 'with confound' condition\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of pandas.DataFrame\n",
    "        Two DataFrames containing the averaged network metrics for both conditions\n",
    "    \"\"\"\n",
    "    # Define columns to process\n",
    "    metric_cols = [\n",
    "        'degree_(AnatomicalVar)', \n",
    "        'betweeness_(AnatomicalVar)', \n",
    "        'eigenvec_(AnatomicalVar)', \n",
    "        'clusteringcoef_(AnatomicalVar)', \n",
    "        'smallworldness(AnatomicalVar)',\n",
    "        'avg_shortestPathLength(AnatomicalVar)'\n",
    "    ]\n",
    "    \n",
    "    # Process a single dataframe\n",
    "    def process_dataframe(df, rename_cols=False):\n",
    "        # Filter the dataframe\n",
    "        filtered_df = df[(df['acquisition'] == 'acq-RL') & \n",
    "                         (df['session'] == '1')] #&(df['iteration'] == \"iter_1\")]\n",
    "        # print(len(filtered_df))\n",
    "        # Dictionary to store stacked data\n",
    "        stacked_data = {}\n",
    "        \n",
    "        # Process each metric column\n",
    "        for col in metric_cols:\n",
    "            stacked_data[col] = np.vstack(filtered_df[col].values)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        stacked_df = pd.DataFrame([stacked_data])\n",
    "        \n",
    "        result_df = pd.DataFrame({\n",
    "            col: [np.sqrt(np.mean(stacked_df.iloc[0][col], axis=0))] for col in metric_cols\n",
    "        })\n",
    "        \n",
    "        # # Add scalar metrics directly\n",
    "        # result_df['smallworldness(AnatomicalVar)'] = stacked_df['smallworldness(AnatomicalVar)']\n",
    "        # result_df['avg_shortestPathLength(AnatomicalVar)'] = stacked_df['avg_shortestPathLength(AnatomicalVar)']\n",
    "        # ✅ check if any NaN exists in the DataFrame\n",
    "        if result_df.isnull().values.any(): \n",
    "            print(\"⚠️ Warning: result_df contains NaN values in some columns\")\n",
    "            # optionally, print which columns\n",
    "            print(\"Columns with NaN:\", result_df.columns[result_df.isnull().any()].tolist())\n",
    "  \n",
    "        # Rename columns if requested (for Wconf data)\n",
    "        if rename_cols:\n",
    "            rename_mapping = {col: col.replace('(AnatomicalVar)', '(AnatomicalVarW)') \n",
    "                             for col in result_df.columns}\n",
    "            result_df = result_df.rename(columns=rename_mapping)\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    # Process both dataframes\n",
    "    avrg_Noconf_anat = process_dataframe(df_Noconf_anat, rename_cols=False)\n",
    "    avrg_Wconf_anat = process_dataframe(df_Wconf_anat, rename_cols=True)\n",
    "    \n",
    "    return avrg_Noconf_anat, avrg_Wconf_anat\n",
    "\n",
    "\n",
    "def compute_average_num(df_Noconf_num, df_Wconf_num):\n",
    "    \"\"\"\n",
    "    Compute average numerical network metrics for 'no confidence' and 'with confidence' conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_Noconf_num : pandas.DataFrame\n",
    "        DataFrame containing numerical network metrics for 'no confidence' condition\n",
    "    df_Wconf_num : pandas.DataFrame\n",
    "        DataFrame containing numerical network metrics for 'with confidence' condition\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple of pandas.DataFrame\n",
    "        Two DataFrames containing the averaged numerical metrics for both conditions\n",
    "    \"\"\"\n",
    "    # Define columns to process\n",
    "    metric_cols = [\n",
    "        'degree_(numericalVar)', \n",
    "        'betweeness_(numericalVar)', \n",
    "        'eigenvec_(numericalVar)', \n",
    "        'clusteringcoef_(numericalVar)', \n",
    "        'smallworldness(numericalVar)',\n",
    "        'avg_shortestPathLength(numericalVar)'\n",
    "    ]\n",
    "    \n",
    "    # Process a single dataframe\n",
    "    def process_dataframe(df, rename_cols=False):\n",
    "        # Filter the dataframe\n",
    "        filtered_df = df[(df['acquisition'] == 'acq-RL') & \n",
    "                         (df['session'] == '1')]\n",
    "        print(len(filtered_df))\n",
    "        # Dictionary to store stacked data\n",
    "        stacked_data = {}\n",
    "        \n",
    "        # Process each metric column\n",
    "        for col in metric_cols:\n",
    "            stacked_data[col] = np.vstack(filtered_df[col].values)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        stacked_df = pd.DataFrame([stacked_data])\n",
    "        \n",
    "\n",
    "        \n",
    "        result_df = pd.DataFrame({\n",
    "            col: [np.sqrt(np.mean(stacked_df.iloc[0][col], axis=0))] for col in metric_cols\n",
    "        })\n",
    "                # ✅ check if any NaN exists in the DataFrame\n",
    "        if result_df.isnull().values.any(): \n",
    "            print(\"⚠️ Warning: result_df contains NaN values in some columns\")\n",
    "            # optionally, print which columns\n",
    "            print(\"Columns with NaN:\", result_df.columns[result_df.isnull().any()].tolist())\n",
    "        # # Add scalar metrics directly\n",
    "        # result_df['smallworldness(numericalVar)'] = stacked_df['smallworldness(numericalVar)']\n",
    "        # result_df['avg_shortestPathLength(numericalVar)'] = stacked_df['avg_shortestPathLength(numericalVar)']\n",
    "        \n",
    "        # Rename columns if requested (for Wconf data)\n",
    "        if rename_cols:\n",
    "            rename_mapping = {col: col.replace('(numericalVar)', '(numericalVarW)') \n",
    "                             for col in result_df.columns}\n",
    "            result_df = result_df.rename(columns=rename_mapping)\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    # Process both dataframes\n",
    "    avrg_Noconf_num = process_dataframe(df_Noconf_num, rename_cols=False)\n",
    "    avrg_Wconf_num = process_dataframe(df_Wconf_num, rename_cols=True)\n",
    "    \n",
    "    return avrg_Noconf_num, avrg_Wconf_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Standard deviation across MCA runs (for each subject, region, and metric):\n",
    "\n",
    "$$\n",
    "\\sigma_{s,r,m} = \\mathrm{StdDev}(X_{s,r,m,1}, X_{s,r,m,2}, \\ldots, X_{s,r,m,10})\n",
    "$$\n",
    "\n",
    "Average standard deviation across subjects (per region and metric):\n",
    "\n",
    "$$\n",
    "\\overline{\\sigma}_{r, m} = \\frac{1}{S} \\sum_{s=1}^{S} \\sigma_{s, r, m}\n",
    "$$\n",
    "Standard deviation across subjects (for each MCA iteration, region, and metric):\n",
    "\n",
    "$$\\sigma_{r,m,iter} = \\mathrm{StdDev}(X_{1,r,m,iter}, X_{2,r,m,iter}, \\ldots, X_{s,r,m,iter})$$\n",
    "\n",
    "Average standard deviation across MCA iteration (per region and metric):\n",
    "\n",
    "$$\\overline{\\sigma}_{r, m} = \\frac{1}{10} \\sum_{iter=1}^{10} \\sigma_{ r, m,iter}$$\n",
    "\n",
    "$$s \\in \\{Subjects\\} , r \\in \\{Regions\\}, m \\in \\{ Metrics \\}, iter \\in \\{1,...,10\\} $$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, avrg1_Wconf_num = compute_average_num(df1_Wconf_num, df1_Wconf_num)\n",
    "_, avrg1_Wconf_anat = compute_average_anat(df1_Wconf_anat, df1_Wconf_anat)\n",
    "\n",
    "\n",
    "\n",
    "_, avrg2_Wconf_num = compute_average_num(df2_Wconf_num, df2_Wconf_num)\n",
    "_, avrg2_Wconf_anat = compute_average_anat(df2_Wconf_anat, df2_Wconf_anat)\n",
    "\n",
    "\n",
    "_, avrg3_Wconf_num = compute_average_num(df3_Wconf_num, df3_Wconf_num)\n",
    "_, avrg3_Wconf_anat = compute_average_anat(df3_Wconf_anat, df3_Wconf_anat)\n",
    "\n",
    "\n",
    "\n",
    "_, avrg4_Wconf_num = compute_average_num(df4_Wconf_num, df4_Wconf_num)\n",
    "_, avrg4_Wconf_anat = compute_average_anat(df4_Wconf_anat, df4_Wconf_anat)\n",
    "\n",
    "\n",
    "\n",
    "_, avrg5_Wconf_num = compute_average_num(df5_Wconf_num, df5_Wconf_num)\n",
    "_, avrg5_Wconf_anat = compute_average_anat(df5_Wconf_anat, df5_Wconf_anat)\n",
    "\n",
    "\n",
    "\n",
    "_, avrg05_Wconf_num = compute_average_num(df05_Wconf_num, df05_Wconf_num)\n",
    "_, avrg05_Wconf_anat = compute_average_anat(df05_Wconf_anat, df05_Wconf_anat)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "atlas = datasets.fetch_atlas_schaefer_2018(n_rois=100, yeo_networks=7)\n",
    "atlas_labels = atlas.labels[1:]\n",
    "atlas_filename=atlas.maps\n",
    "# atlas_labels[0:51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the NPVR for global metrics\n",
    "\n",
    "def cmput_ratio_g(df, df1, N):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    metrics = ['smallworldness', 'avg_shortestPathLength']\n",
    "    # numerical_data = {'subject': df['subject']}\n",
    "    numerical_data = {}\n",
    "    for metric in metrics:\n",
    "        numerical_col = [col for col in df.columns if metric in col and 'numericalVar' in col]\n",
    "        anatomical1_col = [col for col in df1.columns if metric in col and 'AnatomicalVar' in col]\n",
    "\n",
    "        if numerical_col and anatomical1_col :\n",
    "            numerical_col = numerical_col[0]\n",
    "            anatomical_col1 = anatomical1_col[0]\n",
    "\n",
    "            a = df1[anatomical_col1].values[0]  # make sure this is a scalar\n",
    "            # n=(df[numerical_col])\n",
    "            n=(df[numerical_col]).values[0]\n",
    "            ratio_values =((n) / (a)) if a != 0 else print('zero') #(2/np.sqrt(N)) *\n",
    "            numerical_data[f'ratio_{metric}'] = ratio_values\n",
    "        else:\n",
    "            print(f\"Warning: Could not find matching columns for {metric}\")\n",
    "\n",
    "    result_df = pd.DataFrame(numerical_data)\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the NPVR for local metrics\n",
    "\n",
    "def cmput_ratio(df,df1,N):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Dictionary to hold column: list_of_ratios\n",
    "    ratio_data = {}\n",
    "\n",
    "    # List of metric keywords to match columns\n",
    "    metrics = ['degree', 'betweeness', 'eigenvec', 'clusteringcoef']\n",
    "\n",
    "    for metric in metrics:\n",
    "        # Get the matching column names\n",
    "        num_col = [col for col in df.columns if metric in col][0]\n",
    "        anat1_col = [col for col in df1.columns if metric in col][0]\n",
    "        # Extract the values (lists of 100 values)\n",
    "        num_vals = df[num_col].iloc[0]\n",
    "        anat_vals = df1[anat1_col].iloc[0]\n",
    "        # N1 = 147-8\n",
    "        # N2 = 34  # assuming equal sizes\n",
    "\n",
    "        # Flatten if nested in [[x]]\n",
    "        if isinstance(num_vals[0], list):\n",
    "            num_vals = [v[0] for v in num_vals]\n",
    "            anat_vals = [v[0] for v in anat_vals]\n",
    "            # print('num_vals=', num_vals)\n",
    "        # Compute σ_pool for each pair and then n / σ_pool\n",
    "        pooled_ratios = []\n",
    "        for n, a in zip(num_vals, anat_vals):\n",
    "            ratio =(  ((n) / (a)) )if a != 0 else print('zero') #(2/np.sqrt(N)) *\n",
    "            # print(\"n,a,b\",n,a,b)\n",
    "            pooled_ratios.append(ratio)\n",
    "\n",
    "        # Store as a single list under one row\n",
    "        ratio_data[metric + '_ratio'] = [pooled_ratios]\n",
    "\n",
    "    # Convert to single-row DataFrame\n",
    "    ratio_df = pd.DataFrame(ratio_data)\n",
    "\n",
    "    return ratio_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- $$\n",
    "\\bar{\\sigma}(\\text{d}) = \\frac{\\bar{\\sigma}(\\text{num})}{\\bar{\\sigma}_{pool}}\n",
    "$$ -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Variance across MCA runs (for each subject, region, and metric):\n",
    "\n",
    "$$\n",
    "Var_{s,r,m} = \\mathrm{Var}(X_{s,r,m,1}, X_{s,r,m,2}, \\ldots, X_{s,r,m,10})\n",
    "$$\n",
    "\n",
    "Average Variance across subjects (per region and metric):\n",
    "\n",
    "$$\n",
    "\\overline{Var}_{r, m} = \\frac{1}{S} \\sum_{s=1}^{S} Var_{s, r, m}\n",
    "$$\n",
    "$$\n",
    "\\sigma_{num} = \\sqrt{\\overline{Var}_{r, m}}\n",
    "$$\n",
    "\n",
    "Variance  across subjects (for each MCA iteration, region, and metric):\n",
    "\n",
    "$$Var_{r,m,iter} = \\mathrm{Var}(X_{1,r,m,iter}, X_{2,r,m,iter}, \\ldots, X_{s,r,m,iter})$$\n",
    "\n",
    "Average Variance across MCA iteration (per region and metric):\n",
    "\n",
    "$$\\overline{Var}_{r, m} = \\frac{1}{10} \\sum_{iter=1}^{10} Var_{ r, m,iter}$$\n",
    "\n",
    "$$\n",
    "\\sigma_{pool} = \\sqrt{\\overline{Var}_{r, m}}\n",
    "$$\n",
    "\n",
    "\n",
    "$$s \\in \\{Subjects\\} , r \\in \\{Regions\\}, m \\in \\{ Metrics \\}, iter \\in \\{1,...,10\\} $$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio1=cmput_ratio(avrg1_Wconf_num,avrg1_Wconf_anat,182)\n",
    "ratio1G=cmput_ratio_g(avrg1_Wconf_num ,avrg1_Wconf_anat,182)\n",
    "\n",
    "ratio2=cmput_ratio(avrg2_Wconf_num,avrg2_Wconf_anat,179)\n",
    "ratio2G=cmput_ratio_g(avrg2_Wconf_num ,avrg2_Wconf_anat,179)\n",
    "\n",
    "ratio3=cmput_ratio(avrg3_Wconf_num,avrg3_Wconf_anat,176)\n",
    "ratio3G=cmput_ratio_g(avrg3_Wconf_num ,avrg3_Wconf_anat,176)\n",
    "\n",
    "ratio4=cmput_ratio(avrg4_Wconf_num,avrg4_Wconf_anat,170)\n",
    "ratio4G=cmput_ratio_g(avrg4_Wconf_num ,avrg4_Wconf_anat,170)\n",
    "\n",
    "ratio5=cmput_ratio(avrg5_Wconf_num,avrg5_Wconf_anat,147)\n",
    "ratio5G=cmput_ratio_g(avrg5_Wconf_num ,avrg5_Wconf_anat,147)\n",
    "\n",
    "ratio05=cmput_ratio(avrg05_Wconf_num,avrg05_Wconf_anat,182)\n",
    "ratio05G=cmput_ratio_g(avrg05_Wconf_num ,avrg05_Wconf_anat,182)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the average NPVR for local and global metrics\n",
    "\n",
    "avrgRatio05 = {}\n",
    "avrgRatio1 = {}\n",
    "avrgRatio2 = {}\n",
    "avrgRatio3= {}\n",
    "avrgRatio4 = {}\n",
    "avrgRatio5= {}\n",
    "\n",
    "\n",
    "for col in ratio1.columns:\n",
    "    avrgRatio05[col]=np.mean(ratio05[col].values[0])\n",
    "    avrgRatio1[col]=np.mean(ratio1[col].values[0])\n",
    "    avrgRatio2[col]=np.mean(ratio2[col].values[0])\n",
    "    avrgRatio3[col]=np.mean(ratio3[col].values[0])\n",
    "    avrgRatio4[col]=np.mean(ratio4[col].values[0])\n",
    "    avrgRatio5[col]=np.mean(ratio5[col].values[0])\n",
    "   \n",
    "\n",
    "GavrgRatio05 = {}\n",
    "GavrgRatio1 = {}\n",
    "GavrgRatio2 = {}\n",
    "GavrgRatio3= {}\n",
    "GavrgRatio4 = {}\n",
    "GavrgRatio5= {}\n",
    "\n",
    "for col in ratio1G.columns[:]:\n",
    "    GavrgRatio05[col]=np.mean(list(ratio05G[col]))\n",
    "    GavrgRatio1[col]=np.mean(list(ratio1G[col]))\n",
    "    GavrgRatio2[col]=np.mean(list(ratio2G[col]))\n",
    "    GavrgRatio3[col]=np.mean(list(ratio3G[col]))\n",
    "    GavrgRatio4[col]=np.mean(list(ratio4G[col]))\n",
    "    GavrgRatio5[col]=np.mean(list(ratio5G[col]))\n",
    "GavrgRatio1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Compute ratios\n",
    "ratio1 = cmput_ratio(avrg1_Wconf_num, avrg1_Wconf_anat, 182)\n",
    "ratio2 = cmput_ratio(avrg2_Wconf_num, avrg2_Wconf_anat, 179)\n",
    "ratio3 = cmput_ratio(avrg3_Wconf_num, avrg3_Wconf_anat, 176)\n",
    "ratio4 = cmput_ratio(avrg4_Wconf_num, avrg4_Wconf_anat, 170)\n",
    "ratio5 = cmput_ratio(avrg5_Wconf_num, avrg5_Wconf_anat, 147)\n",
    "ratio05 = cmput_ratio(avrg05_Wconf_num, avrg05_Wconf_anat, 182)\n",
    "\n",
    "\n",
    "def change(arr,col):\n",
    "    # Create the 'region' column with labels\n",
    "    regions = [f\"region_{i+1}\" for i in range(len(arr))]\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({'region': regions, f'{col}': arr})\n",
    "    return df\n",
    "# Dictionary mapping threshold digits -> ratio DataFrame\n",
    "ratio_map = {\n",
    "    \"5\": ratio5,\n",
    "    \"4\": ratio4,\n",
    "    \"3\": ratio3,\n",
    "    \"2\": ratio2,\n",
    "    \"1\": ratio1,\n",
    "    \"05\": ratio05\n",
    "}\n",
    "\n",
    "# Thresholds\n",
    "thresholds = [0.5, 0.4, 0.3, 0.2, 0.1, 0.05]  \n",
    "\n",
    "# Metrics\n",
    "metrics = ['betweeness', 'clusteringcoef', 'eigen', 'degree']\n",
    "metric_labels = [\"Betweenness Centrality\", \"Clustering Coefficient\",\n",
    "                 \"Eigenvector Centrality\", \"Degree Centrality\"]\n",
    "\n",
    "# Colors\n",
    "colors = {\"PD\": \"rgb(128, 0, 128)\"}\n",
    "\n",
    "# Subplot grid: 4 rows (metrics) × len(thresholds) cols\n",
    "fig = make_subplots(\n",
    "    rows=len(metrics), cols=len(thresholds),\n",
    "    shared_yaxes='rows',\n",
    "    subplot_titles=[f\"Thr {thr}\" for thr in thresholds]*len(metrics),\n",
    "    vertical_spacing=0.08\n",
    ")\n",
    "\n",
    "# Loop over metrics & thresholds\n",
    "for row, (metric, metric_label) in enumerate(zip(metrics, metric_labels), start=1):\n",
    "    for col, thr in enumerate(thresholds, start=1):\n",
    "        thre = str(thr).split(\".\")[1]   # e.g. 0.05 -> \"05\", 0.5 -> \"5\"\n",
    "        rPD = ratio_map[thre]\n",
    "        # --- Extract values for this metric ---\n",
    "        pd_val = rPD[[c for c in rPD.columns if metric in c]].iloc[0, 0]\n",
    "        \n",
    "        pd_valreg=change(pd_val[:],metric)\n",
    "        pd_vals=pd_valreg.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "        # print(thre,metric,pd_vals)\n",
    "        # Add PD box\n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                y=pd_vals['value'],\n",
    "                name=\"Sample\",\n",
    "                marker=dict(color=colors[\"PD\"], size=4),\n",
    "                boxpoints=\"all\", jitter=0.3, pointpos=-0.4,\n",
    "                width=0.35,\n",
    "                text=pd_vals['region'],\n",
    "                hovertemplate=\"<b>Region:</b> %{text}<br><b>Value:</b> %{y}<br><extra>PD</extra>\",\n",
    "                showlegend=(row==1 and col==1)  # only show legend once\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "\n",
    "    # Add row title (metric label) on y-axis\n",
    "    fig.update_yaxes(title_text=metric_label, row=row, col=1)\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(\n",
    "    height=300*len(metrics),\n",
    "    width=220*len(thresholds),\n",
    "    title=\"Sample variability Ratio of No Confound Pipeline across different metric and absolute thresholds\",\n",
    "    boxmode=\"group\"\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn import image, plotting, datasets\n",
    "import tempfile\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 16,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'legend.fontsize': 14,\n",
    "    'figure.titlesize': 20\n",
    "})\n",
    "# --------------------------\n",
    "# Load Schaefer Atlas (100 regions)\n",
    "# --------------------------\n",
    "atlas = datasets.fetch_atlas_schaefer_2018(n_rois=100, yeo_networks=7)\n",
    "atlas_filename = atlas.maps\n",
    "atlas_labels = atlas.labels[1:]\n",
    "\n",
    "# --------------------------\n",
    "# Your ratio computations (already available)\n",
    "# --------------------------\n",
    "ratio1 = cmput_ratio(avrg1_Wconf_num, avrg1_Wconf_anat, 182)\n",
    "ratio2 = cmput_ratio(avrg2_Wconf_num, avrg2_Wconf_anat, 179)\n",
    "ratio3 = cmput_ratio(avrg3_Wconf_num, avrg3_Wconf_anat, 176)\n",
    "ratio4 = cmput_ratio(avrg4_Wconf_num, avrg4_Wconf_anat, 170)\n",
    "ratio5 = cmput_ratio(avrg5_Wconf_num, avrg5_Wconf_anat, 147)\n",
    "ratio05 = cmput_ratio(avrg05_Wconf_num, avrg05_Wconf_anat, 182)\n",
    "\n",
    "ratio_map = {\n",
    "    \"05\": ratio05,\n",
    "    \"1\": ratio1,\n",
    "    \"2\": ratio2,\n",
    "    \"3\": ratio3,\n",
    "    \"4\": ratio4,\n",
    "    \"5\": ratio5\n",
    "}\n",
    "\n",
    "thresholds =[0.05, 0.1, 0.2, 0.3, 0.4, 0.5] #[0.1,0.4] # [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "threshold_labels = [\"0.05\", \"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\"] #[\"0.1\",\"0.4\"] #[\"0.05\", \"0.1\", \"0.2\", \"0.3\", \"0.4\", \"0.5\"]\n",
    "metrics = ['betweeness', 'clusteringcoef', 'eigen', 'degree']\n",
    "metric_labels = [\"Betweenness Centrality\", \"Clustering Coefficient\", \"Eigenvector Centrality\", \"Degree Centrality\"]\n",
    "\n",
    "# --------------------------\n",
    "# Compute SD table\n",
    "# --------------------------\n",
    "sd_table = pd.DataFrame(columns=['Threshold', 'Metric', 'Std'])\n",
    "\n",
    "for thr in thresholds:\n",
    "    thre = str(thr).split(\".\")[1] if \".\" in str(thr) else str(int(thr))\n",
    "    rPD = ratio_map[thre]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        vals = rPD[[c for c in rPD.columns if metric in c]].iloc[0, 0]\n",
    "        std_val = pd.Series(vals).std()\n",
    "        sd_table.loc[len(sd_table)] = [thr, metric_labels[i], std_val]\n",
    "\n",
    "# --------------------------\n",
    "# Function: map ratios to brain\n",
    "# --------------------------\n",
    "def map_ratio_to_brain(ratio_values, atlas_filename):\n",
    "    atlas_img = image.load_img(atlas_filename)\n",
    "    atlas_data = atlas_img.get_fdata()\n",
    "    ratio_values = np.nan_to_num(ratio_values)\n",
    "    ratio_img_data = np.zeros_like(atlas_data)\n",
    "    for i, val in enumerate(ratio_values, start=1):\n",
    "        ratio_img_data[atlas_data == i] = val\n",
    "    return image.new_img_like(atlas_img, ratio_img_data)\n",
    "\n",
    "# --------------------------\n",
    "# Temporary directory for snapshots\n",
    "# --------------------------\n",
    "tmpdir = tempfile.mkdtemp()\n",
    "\n",
    "# --------------------------\n",
    "# Compute global vmin/vmax for consistent colorbar\n",
    "# --------------------------\n",
    "# all_vals = []\n",
    "# for thr in thresholds:\n",
    "#     thre = str(thr).split(\".\")[1] if \".\" in str(thr) else str(int(thr))\n",
    "#     rPD = ratio_map[thre]\n",
    "#     for metric in metrics:\n",
    "#         vals = rPD[[c for c in rPD.columns if metric in c]].iloc[0, 0]\n",
    "#         all_vals.extend(vals)\n",
    "# vmin, vmax = np.nanmin(all_vals), np.nanmax(all_vals)\n",
    "# --------------------------\n",
    "# Generate brain map images and store paths\n",
    "# --------------------------\n",
    "image_paths = {metric: [] for metric in metrics}\n",
    "\n",
    "for metric in metrics:\n",
    "    for thr in thresholds:\n",
    "        thre = str(thr).split(\".\")[1] if \".\" in str(thr) else str(int(thr))\n",
    "        rPD = ratio_map[thre]\n",
    "        ratio_values = rPD[[c for c in rPD.columns if metric in c]].iloc[0, 0]\n",
    "        # ratio_img = map_ratio_to_brain(ratio_values, atlas_filename)\n",
    "        vmin, vmax = 0,1 #np.nanmin(ratio_values), np.nanmax(ratio_values) #compute vmin,vmax for each metric separately or select  global vmin/vmax\n",
    "        # Normalize (0–1 range)\n",
    "        norm_vals = (ratio_values - np.min(ratio_values)) / (\n",
    "            np.max(ratio_values) - np.min(ratio_values)\n",
    "        )\n",
    "        ratio_img = map_ratio_to_brain(norm_vals, atlas_filename)\n",
    "        out_path = os.path.join(tmpdir, f\"{metric}_thr{thre}.png\")\n",
    "        plotting.plot_stat_map(\n",
    "            ratio_img,\n",
    "            display_mode='ortho', #pick view\n",
    "            cut_coords=[2,0,0],\n",
    "            cmap='Reds',\n",
    "            colorbar=True,\n",
    "            black_bg=False,\n",
    "            dim=0,\n",
    "            threshold=0 ,#vmin - 10**-6,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "            output_file=out_path\n",
    "        )\n",
    "        image_paths[metric].append(out_path)\n",
    "        \n",
    "\n",
    "# --------------------------\n",
    "# Assemble figure grid (metrics as rows, thresholds as columns)\n",
    "# --------------------------\n",
    "n_rows = len(metrics)\n",
    "n_cols = len(thresholds)\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 10))\n",
    "plt.subplots_adjust(wspace=0.01, hspace=0.05, left=0.08, right=0.90, bottom=0.08, top=0.92)\n",
    "\n",
    "for i, metric_label in enumerate(metric_labels):\n",
    "    metric = metrics[i]\n",
    "    for j, thr_label in enumerate(threshold_labels):\n",
    "        img = plt.imread(image_paths[metric][j])\n",
    "        axes[i, j].imshow(img)\n",
    "        axes[i, j].axis(\"off\")\n",
    "\n",
    "        # Column titles: thresholds\n",
    "        if i == 0:\n",
    "            axes[i, j].set_title(f\"Threshold = {thr_label}\", fontsize=11, pad=8)\n",
    "\n",
    "        # Row labels: metrics\n",
    "        if j == 0:\n",
    "            axes[i, j].text(-0.15, 0.5, metric_label,\n",
    "                            transform=axes[i, j].transAxes,\n",
    "                            fontsize=11, va='center', ha='right',\n",
    "                            weight='bold')\n",
    "\n",
    "        # SD annotation\n",
    "        sd_value = sd_table.loc[\n",
    "            (sd_table[\"Threshold\"] == float(thr_label)) &\n",
    "            (sd_table[\"Metric\"] == metric_label),\n",
    "            \"Std\"\n",
    "        ].values[0]\n",
    "        axes[i, j].text(0.5, 0.95, f\"SD = {sd_value:.3f}\",\n",
    "                        transform=axes[i, j].transAxes,\n",
    "                        ha='center', va='top', fontsize=10, color='black')\n",
    "\n",
    "# --------------------------\n",
    "# Shared colorbar\n",
    "# --------------------------\n",
    "# sm = plt.cm.ScalarMappable(cmap='jet', norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "# cbar_ax = fig.add_axes([0.93, 0.15, 0.02, 0.7])\n",
    "# cbar = fig.colorbar(sm, cax=cbar_ax)\n",
    "# cbar.set_label(\"Variability Ratio\", rotation=270, labelpad=15)\n",
    "\n",
    "# --------------------------\n",
    "# Figure title\n",
    "# --------------------------\n",
    "fig.suptitle(\"Variability Ratio of No Confound Pipeline Across Regions for differnt Thresholds (Rows = Metrics, Columns = Thresholds)\", fontsize=14, y=0.99)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read With Confound data pickles\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Folder where pickles were saved\n",
    "folder = \"./pickles\"\n",
    "\n",
    "# File names\n",
    "names_num = [f\"avrg{label}_Wconf_num.pkl\" for label in [\"05\", \"1\", \"2\", \"3\", \"4\", \"5\"]]\n",
    "names_anat = [f\"avrg{label}_Wconf_anat.pkl\" for label in [\"05\", \"1\", \"2\", \"3\", \"4\", \"5\"]]\n",
    "names_ratio = [f\"avrgRatio{label}.pkl\" for label in [\"05\", \"1\", \"2\", \"3\", \"4\", \"5\"]]\n",
    "names_ratioG = [f\"ratio{label}G.pkl\" for label in [\"05\", \"1\", \"2\", \"3\", \"4\", \"5\"]]\n",
    "\n",
    "# Combine them all\n",
    "all_files = names_num + names_anat + names_ratio+names_ratioG\n",
    "\n",
    "# ✅ Load everything with correct format and add \"W\" prefix\n",
    "for name in all_files:\n",
    "    path = os.path.join(folder, name)\n",
    "    var_name = \"W\" + os.path.splitext(name)[0]  # e.g., avrg05_Wconf_num.pkl → Wavrg05_Wconf_num\n",
    "\n",
    "    # Dictionaries are only in avrgRatio files\n",
    "    if \"Ratio\" in name:\n",
    "        with open(path, \"rb\") as f:\n",
    "            globals()[var_name] = pickle.load(f)\n",
    "    else:\n",
    "        globals()[var_name] = pd.read_pickle(path)\n",
    "\n",
    "    print(f\"✅ Loaded {var_name} ({type(globals()[var_name]).__name__})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute NPVR for With Confound Pipelines\n",
    "Wratio1=cmput_ratio(Wavrg1_Wconf_num,Wavrg1_Wconf_anat,182)\n",
    "\n",
    "Wratio2=cmput_ratio(Wavrg2_Wconf_num,Wavrg2_Wconf_anat,179)\n",
    "\n",
    "Wratio3=cmput_ratio(Wavrg3_Wconf_num,Wavrg3_Wconf_anat,176)\n",
    "\n",
    "Wratio4=cmput_ratio(Wavrg4_Wconf_num,Wavrg4_Wconf_anat,170)\n",
    "\n",
    "Wratio5=cmput_ratio(Wavrg5_Wconf_num,Wavrg5_Wconf_anat,147)\n",
    "\n",
    "Wratio05=cmput_ratio(Wavrg05_Wconf_num,Wavrg05_Wconf_anat,182)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Differences between NoConfound and WithConfound Data\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Helper function\n",
    "# -------------------------------\n",
    "\n",
    "def compute_differences(list_1, list_2):\n",
    "    \"\"\"\n",
    "    Compute difference between paired DataFrames in two lists.\n",
    "    Each resulting DataFrame keeps the same column names.\n",
    "    Returns a list of DataFrames.\n",
    "    \"\"\"\n",
    "    diffs = []\n",
    "    for df_1, df_2 in zip(list_1, list_2):\n",
    "        diff_df = pd.DataFrame(index=df_1.index)\n",
    "        for col in df_1.columns:\n",
    "            # Subtract arrays elementwise\n",
    "            diff_df[col] = df_1[col].apply(np.array) - df_2[col].apply(np.array)\n",
    "        diffs.append(diff_df)\n",
    "    return diffs\n",
    "\n",
    "\n",
    "def compute_avg_ratio_differences(list_1, list_2):\n",
    "    \"\"\"\n",
    "    Compute difference between dictionaries of ratios (scalar values).\n",
    "    Returns list of dicts.\n",
    "    \"\"\"\n",
    "    diff_dicts = []\n",
    "    for l1, l2 in zip(list_1, list_2):\n",
    "        diff = {k: l1[k] - l2[k] for k in l1 if k in l2}\n",
    "        diff_dicts.append(diff)\n",
    "    return diff_dicts\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Compute all differences\n",
    "# -------------------------------\n",
    "\n",
    "# NumericalVar\n",
    "dataframes_num = [avrg05_Wconf_num, avrg1_Wconf_num, avrg2_Wconf_num, avrg3_Wconf_num, avrg4_Wconf_num, avrg5_Wconf_num]\n",
    "dataframes_anat = [Wavrg05_Wconf_num, Wavrg1_Wconf_num, Wavrg2_Wconf_num, Wavrg3_Wconf_num, Wavrg4_Wconf_num, Wavrg5_Wconf_num]\n",
    "\n",
    "# SampleVar\n",
    "dataframes_pd = [avrg05_Wconf_anat, avrg1_Wconf_anat, avrg2_Wconf_anat, avrg3_Wconf_anat, avrg4_Wconf_anat, avrg5_Wconf_anat]\n",
    "dataframes_hc = [Wavrg05_Wconf_anat, Wavrg1_Wconf_anat, Wavrg2_Wconf_anat, Wavrg3_Wconf_anat, Wavrg4_Wconf_anat, Wavrg5_Wconf_anat]\n",
    "\n",
    "# RatioVar\n",
    "Wratio = [Wratio05, Wratio1, Wratio2, Wratio3, Wratio4, Wratio5]\n",
    "Nratio = [ratio05, ratio1, ratio2, ratio3, ratio4, ratio5]\n",
    "\n",
    "# Average ratio dicts\n",
    "dataframes_avrRatioPD = [avrgRatio05, avrgRatio1, avrgRatio2, avrgRatio3, avrgRatio4, avrgRatio5]\n",
    "dataframes_avrRatioHC = [WavrgRatio05, WavrgRatio1, WavrgRatio2, WavrgRatio3, WavrgRatio4, WavrgRatio5]\n",
    "\n",
    "# --- Compute differences per threshold ---\n",
    "diff_num = compute_differences(dataframes_num, dataframes_anat)\n",
    "diff_anat = compute_differences(dataframes_pd, dataframes_hc)\n",
    "diff_ratio = compute_differences(Nratio, Wratio)\n",
    "diff_avgratio = compute_avg_ratio_differences(dataframes_avrRatioPD, dataframes_avrRatioHC)\n",
    "\n",
    "# Optional: unpack for clarity\n",
    "diff_num05, diff_num1, diff_num2, diff_num3, diff_num4, diff_num5 = diff_num\n",
    "diff_anat05, diff_anat1, diff_anat2, diff_anat3, diff_anat4, diff_anat5 = diff_anat\n",
    "diff_ratio05, diff_ratio1, diff_ratio2, diff_ratio3, diff_ratio4, diff_ratio5 = diff_ratio\n",
    "diff_avgratio05, diff_avgratio1, diff_avgratio2, diff_avgratio3, diff_avgratio4, diff_avgratio5 = diff_avgratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Differences between NoConfound and WithConfound Data for Local Metrics\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "p_value_matrix = [[0.0024, 0.0024, 0.0024, 0.0024],\n",
    " [0.0144, 1.0,     0.0024, 1.0    ],\n",
    " [0.0024, 0.0024, 0.0024, 0.0024],\n",
    " [0.0024, 0.0024, 0.0024, 0.0024],\n",
    " [0.0024, 0.0024, 0.0024, 0.0024],\n",
    " [0.0024, 0.2712, 0.0024, 0.0024]]\n",
    "def change(arr, col):\n",
    "    regions = [f\"region_{i+1}\" for i in range(len(arr))]\n",
    "    df = pd.DataFrame({'region': regions, f'{col}': arr})\n",
    "    return df\n",
    "\n",
    "# ---- Setup ----\n",
    "metrics = ['degree', 'clusteringcoef', 'eigenvec', 'betweeness']\n",
    "metric_labels = [\"Degree Centrality\", \"Clustering Coefficient\", \"EigenVector Centrality\", \"Betweeness Centrality\"]\n",
    "thresholds = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Replace with your actual dfs\n",
    "dataframes_num = [diff_num05, diff_num1, diff_num2, diff_num3, diff_num4, diff_num5 ]\n",
    "dataframes_anat = [diff_anat05, diff_anat1, diff_anat2, diff_anat3, diff_anat4, diff_anat5 ]\n",
    "dataframes_pd = [diff_ratio05, diff_ratio1, diff_ratio2, diff_ratio3, diff_ratio4, diff_ratio5 ]\n",
    "dataframes_avrRatioPD =[diff_avgratio05, diff_avgratio1, diff_avgratio2, diff_avgratio3, diff_avgratio4, diff_avgratio5 ]\n",
    "\n",
    "# Colors\n",
    "colors_num = px.colors.sequential.Greens[3:9]   # PD numerical\n",
    "colors_anat = px.colors.sequential.Oranges[3:9] # HC numerical\n",
    "colors_pd = px.colors.sequential.Purples[3:9]   # PD anatomical\n",
    "colors_hc = px.colors.sequential.Reds[3:9]      # HC anatomical\n",
    "\n",
    "# ---- Calculate primary axis ranges (NV-PV) ----\n",
    "metric_ranges = {}\n",
    "for metric in metrics:\n",
    "    all_values = []\n",
    "    for df_num, df_anat in zip(dataframes_num, dataframes_anat):\n",
    "        for df in [df_num, df_anat]:\n",
    "            cols = [c for c in df.columns if metric in c]\n",
    "            if cols:\n",
    "                tmp = change(df[cols].iloc[0][cols[0]], cols[0])\n",
    "                melted = tmp.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "                all_values.extend(melted['value'].tolist())\n",
    "    if all_values:\n",
    "        lim = max(abs(min(all_values)), abs(max(all_values)))\n",
    "        metric_ranges[metric] = (-lim, lim)\n",
    "\n",
    "# ---- Calculate secondary axis ranges (NPVR) ----\n",
    "metric_2ranges = {}\n",
    "for metric in metrics:\n",
    "    all_values = []\n",
    "    for df_ratio in dataframes_pd:\n",
    "        cols = [c for c in df_ratio.columns if metric in c]\n",
    "        if cols:\n",
    "            tmp = change(df_ratio[cols].iloc[0][cols[0]], cols[0])\n",
    "            melted = tmp.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "            all_values.extend(melted['value'].tolist())\n",
    "    if all_values:\n",
    "        lim = max(abs(min(all_values)), abs(max(all_values)))\n",
    "        metric_2ranges[metric] = (-lim, lim)\n",
    "\n",
    "# ---- Create subplot ----\n",
    "fig_combined = make_subplots(\n",
    "    rows=4, cols=1,\n",
    "    vertical_spacing=0.04,\n",
    "    specs=[[{\"secondary_y\": True}] for _ in range(4)],\n",
    "    subplot_titles=[f\"<b>{label}</b>\" for label in metric_labels]\n",
    ")\n",
    "\n",
    "legend_added = {'num': False, 'anat': False, 'Ratio': False, 'diffavr_ratio': False, 'diffavr_trend': False}\n",
    "\n",
    "# ---- Bracket function ----\n",
    "def add_significance_bracket(fig, row, group1_x, group2_x, y_position, significance_text):\n",
    "    bracket_height = (y_position * 0.02) if y_position != 0 else 0.01\n",
    "    fig.add_shape(type=\"line\", x0=group1_x, y0=y_position, x1=group2_x, y1=y_position,\n",
    "                  line=dict(color=\"black\", width=2), row=row, col=1)\n",
    "    fig.add_shape(type=\"line\", x0=group1_x, y0=y_position, x1=group1_x, y1=y_position - bracket_height,\n",
    "                  line=dict(color=\"black\", width=2), row=row, col=1)\n",
    "    fig.add_shape(type=\"line\", x0=group2_x, y0=y_position, x1=group2_x, y1=y_position - bracket_height,\n",
    "                  line=dict(color=\"black\", width=2), row=row, col=1)\n",
    "    fig.add_annotation(x=(group1_x+group2_x)/2, y=y_position+bracket_height,\n",
    "                       text=f\"<b>{significance_text}</b>\", showarrow=False,\n",
    "                       font=dict(size=12, color=\"black\"), row=row, col=1)\n",
    "\n",
    "# ---- Loop and plot ----\n",
    "for row, (metric, metric_label) in enumerate(zip(metrics, metric_labels)):\n",
    "    pd_avg_ratio_values, x_positions = [], []\n",
    "\n",
    "    for col_idx, (threshold, df_num, df_anat, df_pd, df_avr_pd) in enumerate(zip(\n",
    "        thresholds, dataframes_num, dataframes_anat, dataframes_pd, dataframes_avrRatioPD\n",
    "    )):\n",
    "        group_center = col_idx * 6\n",
    "        x_num, x_anat = group_center - 1.5, group_center - 0.5\n",
    "        x_pd, x_hc = group_center + 0.5, group_center + 1.5\n",
    "        x_star = group_center\n",
    "\n",
    "        # 1) PD Numerical Variability\n",
    "        cols = [c for c in df_num.columns if metric in c]\n",
    "        if cols:\n",
    "            tmp = change(df_num[cols].iloc[0][cols[0]], cols[0])\n",
    "            melted = tmp.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "            fig_combined.add_trace(go.Box(\n",
    "                x=[x_num]*len(melted), y=melted['value'],\n",
    "                name=\"(Numerical Variability (NV), σ_num)\" if not legend_added['num'] else \"\",\n",
    "                boxpoints='all', jitter=0.2, pointpos=-0.3, width=0.5,\n",
    "                marker=dict(color='blue', size=4),\n",
    "                legendgroup=\"num\", showlegend=(not legend_added['num']),\n",
    "                text=melted['region']\n",
    "            ), row=row+1, col=1)\n",
    "            legend_added['num'] = True\n",
    "\n",
    "        # 2) HC Numerical Variability\n",
    "        cols = [c for c in df_anat.columns if metric in c]\n",
    "        if cols:\n",
    "            tmp = change(df_anat[cols].iloc[0][cols[0]], cols[0])\n",
    "            melted = tmp.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "            fig_combined.add_trace(go.Box(\n",
    "                x=[x_anat]*len(melted), y=melted['value'],\n",
    "                name=\"(Population Variability (PV), σ_pop)\" if not legend_added['anat'] else \"\",\n",
    "                boxpoints='all', jitter=0.2, pointpos=-0.3, width=0.5,\n",
    "                marker=dict(color='orange', size=4),\n",
    "                legendgroup=\"anat\", showlegend=(not legend_added['anat']),\n",
    "                text=melted['region']\n",
    "            ), row=row+1, col=1)\n",
    "            legend_added['anat'] = True\n",
    "\n",
    "        # 3) Ratio Anatomical Variability\n",
    "        cols = [c for c in df_pd.columns if metric in c]\n",
    "        if cols:\n",
    "            tmp = change(df_pd[cols].iloc[0][cols[0]], cols[0])\n",
    "            melted = tmp.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "            fig_combined.add_trace(go.Box(\n",
    "                x=[x_pd]*len(melted), y=melted['value'],\n",
    "                name=\"(NPVR,  ν_npv)\" if not legend_added['Ratio'] else \"\",\n",
    "                boxpoints='all', jitter=0.2, pointpos=-0.3, width=0.5,\n",
    "                marker=dict(color='red', size=4),\n",
    "                legendgroup=\"Ratio\", showlegend=(not legend_added['Ratio']),\n",
    "                text=melted['region']\n",
    "            ), row=row+1, col=1, secondary_y=True)\n",
    "            legend_added['Ratio'] = True\n",
    "            # ---- Add significance star above purple NPVR boxplot ----\n",
    "            # p_value should be a scalar for this metric & threshold\n",
    "            # Example: p_value_matrix[metric_index][threshold_index]\n",
    "            p_value = p_value_matrix[col_idx][row]  # <-- You must point to your actual p-values\n",
    "\n",
    "            # Choose significance level\n",
    "            if p_value < 0.001:\n",
    "                sig_star = \"***\"\n",
    "            elif p_value < 0.01:\n",
    "                sig_star = \"**\"\n",
    "            elif p_value < 0.05:\n",
    "                sig_star = \"*\"\n",
    "            else:\n",
    "                sig_star = \"ns\"\n",
    "\n",
    "            if sig_star:\n",
    "                # Maximum value of this purple NPVR box\n",
    "                max_val = melted['value'].max()\n",
    "\n",
    "                # Place star slightly under\n",
    "                y_star = max_val - 0.05 * abs(max_val)\n",
    "\n",
    "                fig_combined.add_annotation(\n",
    "                    x=x_pd,\n",
    "                    y=y_star,\n",
    "                    text=f\"<b>{sig_star}</b>\",\n",
    "                    showarrow=False,\n",
    "                    font=dict(size=24, color=\"black\"),\n",
    "                    row=row+1, col=1,\n",
    "                    secondary_y=True\n",
    "                )\n",
    "\n",
    "\n",
    "        # Average ratio stars\n",
    "        matching_keys_pd = [k for k in df_avr_pd.keys() if metric in k]\n",
    "        if matching_keys_pd:\n",
    "            pd_avg_val = df_avr_pd[matching_keys_pd[0]]\n",
    "            if not np.isnan(pd_avg_val):\n",
    "                pd_avg_ratio_values.append(pd_avg_val)\n",
    "                x_positions.append(x_star)\n",
    "                fig_combined.add_trace(go.Scatter(\n",
    "                    x=[x_star], y=[pd_avg_val], mode='markers',\n",
    "                    marker=dict(symbol='star', size=20, color='red', line=dict(color='black', width=1)),\n",
    "                    name=\"Average NPVR\" if not legend_added['diffavr_ratio'] else \"\",\n",
    "                    legendgroup=\"diffavr_ratio\", showlegend=(not legend_added['diffavr_ratio']),\n",
    "                ), row=row+1, col=1, secondary_y=True)\n",
    "                legend_added['diffavr_ratio'] = True\n",
    "\n",
    "    # Trend lines\n",
    "    if len(pd_avg_ratio_values) > 1 and len(x_positions) > 1:\n",
    "        fig_combined.add_trace(go.Scatter(\n",
    "            x=x_positions, y=pd_avg_ratio_values, mode='lines',\n",
    "            line=dict(color='red', width=2, dash='dash'),\n",
    "            name=\"diffavr Trend\", legendgroup=\"diffavr_trend\", showlegend=False\n",
    "        ), row=row+1, col=1, secondary_y=True)\n",
    "\n",
    "# ---- Layout ----\n",
    "fig_combined.update_layout(\n",
    "    height=3500, width=2000,\n",
    "    showlegend=True,\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.05,\n",
    "                xanchor=\"center\", x=0.5, font=dict(size=26)),\n",
    "    template='plotly_white',\n",
    "    font=dict(family=\"Arial\", size=20, color=\"black\")\n",
    ")\n",
    "fig_combined.add_annotation(\n",
    "    text=\"<b>*</b> p < 0.05  <b>**</b> p < 0.01  <b>***</b> p < 0.001  ns = not significant\",\n",
    "    showarrow=False,\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.5, y=-0.12,\n",
    "    font=dict(size=26),\n",
    "    xanchor=\"center\"\n",
    ")\n",
    "# Subplot titles\n",
    "for ann in fig_combined.layout.annotations:\n",
    "    ann.font.size = 30\n",
    "\n",
    "# ---- X-axis ----\n",
    "x_tick_positions = [i * 6 for i in range(len(thresholds))]\n",
    "x_tick_labels = [f\"<b>T={t}</b>\" for t in thresholds]\n",
    "for r in range(1, 5):\n",
    "    fig_combined.update_xaxes(\n",
    "        row=r, col=1,\n",
    "        tickvals=x_tick_positions,\n",
    "        ticktext=x_tick_labels if r == 4 else [\"\"]*len(thresholds),\n",
    "        range=[-2, (len(thresholds)-1) * 6 + 2]\n",
    "    )\n",
    "fig_combined.update_xaxes(\n",
    "    row=4, col=1,\n",
    "    title=dict(text=\"<b>Threshold Values</b>\", font=dict(size=30), standoff=20)\n",
    ")\n",
    "\n",
    "# ---- Align Y-axes zero ----\n",
    "for metric in metrics:\n",
    "    row_index = metrics.index(metric) + 1\n",
    "    primary_range = metric_ranges[metric]\n",
    "    secondary_range = metric_2ranges[metric]\n",
    "\n",
    "    # Update primary y-axis\n",
    "    fig_combined.update_yaxes(\n",
    "        row=row_index, col=1,\n",
    "        range=primary_range,\n",
    "        secondary_y=False,\n",
    "        title=dict(text=\"<b>NV-PV Differences</b>\", font=dict(size=28)),\n",
    "        tickfont=dict(size=32)\n",
    "    )\n",
    "\n",
    "    # Update secondary y-axis\n",
    "    fig_combined.update_yaxes(\n",
    "        row=row_index, col=1,\n",
    "        range=secondary_range,\n",
    "        secondary_y=True,\n",
    "        title=dict(text=\"<b>NPVR Difference</b>\", font=dict(size=28)),\n",
    "        tickfont=dict(size=32)\n",
    "    )\n",
    "    fig_combined.update_xaxes(\n",
    "    row=4, col=1,\n",
    "    title=dict(\n",
    "        text=\"<b>Threshold Values</b><br>\"\n",
    "             \"<span style='font-size:28px'>\"\n",
    "             \"*: p<0.05;  **: p<0.01;  *** p<0.001;  ns=not significant\"\n",
    "             \"</span>\"\n",
    "    ),\n",
    "    title_standoff=30\n",
    "\n",
    ")\n",
    "fig_combined.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot Differences between NoConfound and WithConfound Data for Global Metrics\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---- Setup ----\n",
    "metrics = ['smallworldness', 'avg_shortestPathLength']\n",
    "metric_labels = ['Small-worldness', 'Avg Shortest Path Length']\n",
    "thresholds = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Replace with your actual dfs\n",
    "dataframes_num = [avrg05_Wconf_num, avrg1_Wconf_num, avrg2_Wconf_num, avrg3_Wconf_num, avrg4_Wconf_num, avrg5_Wconf_num]\n",
    "dataframes_anat = [Wavrg05_Wconf_num, Wavrg1_Wconf_num, Wavrg2_Wconf_num, Wavrg3_Wconf_num, Wavrg4_Wconf_num, Wavrg5_Wconf_num]\n",
    "dataframes_pd = [avrg05_Wconf_anat, avrg1_Wconf_anat, avrg2_Wconf_anat, avrg3_Wconf_anat, avrg4_Wconf_anat, avrg5_Wconf_anat]\n",
    "dataframes_hc = [Wavrg05_Wconf_anat, Wavrg1_Wconf_anat, Wavrg2_Wconf_anat, Wavrg3_Wconf_anat, Wavrg4_Wconf_anat, Wavrg5_Wconf_anat]\n",
    "\n",
    "dataframesR_num = [ratio05G, ratio1G, ratio2G, ratio3G, ratio4G, ratio5G]\n",
    "dataframesR_anat = [Wratio05G, Wratio1G, Wratio2G, Wratio3G, Wratio4G, Wratio5G]\n",
    "\n",
    "# Helper function\n",
    "def extract_metric_value(df, metric):\n",
    "    try:\n",
    "        if isinstance(df, dict):\n",
    "            val = df.get(metric, np.nan)\n",
    "        else:\n",
    "            if hasattr(df, 'loc'):\n",
    "                if metric in df.index:\n",
    "                    val = df.loc[metric]\n",
    "                elif metric in df.columns:\n",
    "                    val = df[metric]\n",
    "                else:\n",
    "                    matching_cols = [col for col in df.columns if metric in str(col)]\n",
    "                    val = df[matching_cols[0]] if matching_cols else np.nan\n",
    "            else:\n",
    "                val = df\n",
    "\n",
    "        if hasattr(val, 'iloc'):\n",
    "            val = val.iloc[0] if len(val) == 1 else val.mean()\n",
    "        elif hasattr(val, 'values'):\n",
    "            val = val.values[0] if len(val.values) == 1 else np.mean(val.values)\n",
    "\n",
    "        return float(val) if not pd.isna(val) else np.nan\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# --------------------------\n",
    "# CUSTOM LEFT AXIS RANGES YOU REQUESTED\n",
    "# --------------------------\n",
    "custom_y_ranges = {\n",
    "    'smallworldness': (-0.1, 0.05),\n",
    "    'avg_shortestPathLength': (-0.07, 0.017)\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# CREATE SUBPLOTS\n",
    "# --------------------------\n",
    "fig = make_subplots(\n",
    "    rows=len(metrics), cols=1,\n",
    "    subplot_titles=[f\"<b>{label}</b>\" for label in metric_labels],\n",
    "    specs=[[{\"secondary_y\": True}] for _ in range(len(metrics))],\n",
    "    vertical_spacing=0.15\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# PROCESS EACH METRIC\n",
    "# --------------------------\n",
    "for metric_idx, metric in enumerate(metrics):\n",
    "    row = metric_idx + 1\n",
    "\n",
    "    diff_num, diff_anat = [], []\n",
    "    ratio_num, ratio_anat = [], []\n",
    "\n",
    "    for thresh_idx, threshold in enumerate(thresholds):\n",
    "        num_val = extract_metric_value(dataframes_num[thresh_idx], metric)\n",
    "        anat_val = extract_metric_value(dataframes_anat[thresh_idx], metric)\n",
    "        pd_val = extract_metric_value(dataframes_pd[thresh_idx], metric)\n",
    "        hc_val = extract_metric_value(dataframes_hc[thresh_idx], metric)\n",
    "        ratio_pd = extract_metric_value(dataframesR_num[thresh_idx], metric)\n",
    "        ratio_hc = extract_metric_value(dataframesR_anat[thresh_idx], metric)\n",
    "\n",
    "        diff_num.append(num_val - anat_val)\n",
    "        diff_anat.append(pd_val - hc_val)\n",
    "        ratio_num.append(ratio_pd)\n",
    "        ratio_anat.append(ratio_hc)\n",
    "\n",
    "    # Ratio difference\n",
    "    ratio_diff = np.array(ratio_num) - np.array(ratio_anat)\n",
    "\n",
    "    # Small offsets to avoid overlap\n",
    "    x_num = [t - 0.01 for t in thresholds]\n",
    "    x_anat = [t + 0.01 for t in thresholds]\n",
    "\n",
    "    # ===============================\n",
    "    # PLOT: NV-PV DIFFERENCE (NUMERICAL)\n",
    "    # ===============================\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_num, y=diff_num,\n",
    "            mode='markers',\n",
    "            name='(Numerical Variability (NV), σ_num)',\n",
    "            marker=dict(color='blue', size=12, symbol='circle'),\n",
    "            line=dict(color='green', width=2),\n",
    "            showlegend=(metric_idx == 0)\n",
    "        ),\n",
    "        row=row, col=1\n",
    "    )\n",
    "\n",
    "    # ===============================\n",
    "    # PLOT: NV-PV DIFFERENCE (POPULATION)\n",
    "    # ===============================\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x_anat, y=diff_anat,\n",
    "            mode='markers',\n",
    "            name='(Population Variability (PV), σ_pop)',\n",
    "            marker=dict(color='orange', size=12, symbol='circle'),\n",
    "            line=dict(color='orange', width=2),\n",
    "            showlegend=(metric_idx == 0)\n",
    "        ),\n",
    "        row=row, col=1\n",
    "    )\n",
    "\n",
    "    # ===============================\n",
    "    # PLOT: NPVR DIFFERENCE\n",
    "    # ===============================\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=thresholds, y=ratio_diff,\n",
    "            mode='markers+lines',\n",
    "            name='(NPVR, ν_npv)',\n",
    "            marker=dict(color='red', size=14, symbol='star'),\n",
    "            line=dict(color='red', dash='dash', width=2),\n",
    "            showlegend=(metric_idx == 0)\n",
    "        ),\n",
    "        row=row, col=1, secondary_y=True\n",
    "    )\n",
    "\n",
    "# --------------------------\n",
    "# LAYOUT\n",
    "# --------------------------\n",
    "fig.update_layout(\n",
    "    showlegend=True,\n",
    "    height=500 * len(metrics),\n",
    "    width=1800,\n",
    "    template='plotly_white',\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=-0.2,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5,\n",
    "        font=dict(size=26)\n",
    "    )\n",
    ")\n",
    "\n",
    "for ann in fig.layout.annotations:\n",
    "    ann.font.size = 30\n",
    "\n",
    "# --------------------------\n",
    "# AXES (INCLUDING FIXED RANGES)\n",
    "# --------------------------\n",
    "for i, metric in enumerate(metrics):\n",
    "    row = i + 1\n",
    "\n",
    "    # LEFT Y-AXIS (NV-PV DIFFS)\n",
    "    fig.update_yaxes(\n",
    "        title=dict(text=\"<b>NV-PV Differences</b>\", font=dict(size=26)),\n",
    "        tickfont=dict(size=32),\n",
    "        row=row, col=1,\n",
    "        range=custom_y_ranges.get(metric, None)\n",
    "    )\n",
    "\n",
    "    # RIGHT Y-AXIS (NPVR DIFFS)\n",
    "    fig.update_yaxes(\n",
    "        title=dict(text=\"<b>NPVR Differences</b>\", font=dict(size=26)),\n",
    "        tickfont=dict(size=32),\n",
    "        row=row, col=1, secondary_y=True\n",
    "    )\n",
    "\n",
    "    # X-AXIS\n",
    "    fig.update_xaxes(\n",
    "        tickvals=thresholds,\n",
    "        ticktext=[f\"<b>T={t}</b>\" for t in thresholds] if i == len(metrics)-1 else [''] * len(thresholds),\n",
    "        tickfont=dict(size=20),\n",
    "        row=row, col=1\n",
    "    )\n",
    "\n",
    "# Add x-axis label only on bottom\n",
    "fig.update_xaxes(\n",
    "    row=len(metrics), col=1,\n",
    "    title=dict(text=\"<b>Threshold Values</b>\", font=dict(size=30), standoff=20)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Data ----\n",
    "data_num_noconf = [avrg05_Wconf_num, avrg1_Wconf_num, avrg2_Wconf_num, avrg3_Wconf_num, avrg4_Wconf_num, avrg5_Wconf_num]\n",
    "data_num_withconf = [Wavrg05_Wconf_num, Wavrg1_Wconf_num, Wavrg2_Wconf_num, Wavrg3_Wconf_num, Wavrg4_Wconf_num, Wavrg5_Wconf_num]\n",
    "\n",
    "data_anat_noconf = [avrg05_Wconf_anat, avrg1_Wconf_anat, avrg2_Wconf_anat, avrg3_Wconf_anat, avrg4_Wconf_anat, avrg5_Wconf_anat]\n",
    "data_anat_withconf = [Wavrg05_Wconf_anat, Wavrg1_Wconf_anat, Wavrg2_Wconf_anat, Wavrg3_Wconf_anat, Wavrg4_Wconf_anat, Wavrg5_Wconf_anat]\n",
    "\n",
    "\n",
    "# RatioVar\n",
    "Wratio = [Wratio05, Wratio1, Wratio2, Wratio3, Wratio4, Wratio5]\n",
    "Nratio = [ratio05, ratio1, ratio2, ratio3, ratio4, ratio5]\n",
    "\n",
    "# Replace with your actual dfs\n",
    "dataframes_num = [diff_num05, diff_num1, diff_num2, diff_num3, diff_num4, diff_num5 ]\n",
    "dataframes_anat = [diff_anat05, diff_anat1, diff_anat2, diff_anat3, diff_anat4, diff_anat5 ]\n",
    "dataframes_pd = [diff_ratio05, diff_ratio1, diff_ratio2, diff_ratio3, diff_ratio4, diff_ratio5 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multitest import multipletests\n",
    "import scipy.stats\n",
    "from scipy.stats import permutation_test\n",
    "\n",
    "\n",
    "metrics = [\"degree_ratio\",\"clusteringcoef_ratio\",\"eigenvec_ratio\",\"betweeness_ratio\"]\n",
    "thresholds = [0.05,0.1,0.2,0.3,0.4,0.5]\n",
    "dataframes_pd = [diff_ratio05, diff_ratio1, diff_ratio2, diff_ratio3, diff_ratio4, diff_ratio5]\n",
    "\n",
    "raw_pvals = []\n",
    "\n",
    "# First loop: run tests and collect raw p-values\n",
    "for df in dataframes_pd:\n",
    "    for metric in metrics:\n",
    "        data = df[metric].values\n",
    "        # permutation test\n",
    "        res = permutation_test(\n",
    "            (data[0],),\n",
    "            np.mean,\n",
    "            permutation_type='samples',\n",
    "            vectorized=True,\n",
    "            alternative='less',\n",
    "            random_state=1,\n",
    "            axis=0\n",
    "        )\n",
    "        raw_pvals.append(res.pvalue)\n",
    "# Apply Bonferroni correction\n",
    "reject, pvals_bonf, _, _ = multipletests(raw_pvals, method='bonferroni')\n",
    "pvals_bonf_matrix = pvals_bonf.reshape(len(dataframes_pd), len(metrics))\n",
    "print(pvals_bonf_matrix)\n",
    "\n",
    "# Second loop: plot\n",
    "fig, axes = plt.subplots(len(metrics), len(thresholds),\n",
    "                         figsize=(18, 10), sharex=True, sharey=True)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    for j, thr in enumerate(thresholds):\n",
    "        ax = axes[i, j]\n",
    "        df = dataframes_pd[j]\n",
    "        data = df[metric].values\n",
    "        \n",
    "        # permutation test\n",
    "        res = permutation_test(\n",
    "            (data[0],),\n",
    "            np.mean,\n",
    "            permutation_type='samples',\n",
    "            vectorized=True,\n",
    "            alternative='less',\n",
    "            random_state=1,\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        ax.hist(res.null_distribution, bins=20)\n",
    "        ax.axvline(res.statistic, color='red', linestyle='--')\n",
    "\n",
    "        # Add corrected p-value\n",
    "        p_corr = pvals_bonf_matrix[j, i]\n",
    "        ax.text(0.05, 0.9, f\"p={p_corr:.3e}\", transform=ax.transAxes, fontsize=8)\n",
    "\n",
    "        # Set labels only on left column or bottom row\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(metric)\n",
    "        if i == len(metrics)-1:\n",
    "            ax.set_xlabel(f\"thr={thr}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Data ----\n",
    "data_num_noconf = [avrg05_Wconf_num, avrg1_Wconf_num, avrg2_Wconf_num, avrg3_Wconf_num, avrg4_Wconf_num, avrg5_Wconf_num]\n",
    "data_num_withconf = [Wavrg05_Wconf_num, Wavrg1_Wconf_num, Wavrg2_Wconf_num, Wavrg3_Wconf_num, Wavrg4_Wconf_num, Wavrg5_Wconf_num]\n",
    "\n",
    "data_anat_noconf = [avrg05_Wconf_anat, avrg1_Wconf_anat, avrg2_Wconf_anat, avrg3_Wconf_anat, avrg4_Wconf_anat, avrg5_Wconf_anat]\n",
    "data_anat_withconf = [Wavrg05_Wconf_anat, Wavrg1_Wconf_anat, Wavrg2_Wconf_anat, Wavrg3_Wconf_anat, Wavrg4_Wconf_anat, Wavrg5_Wconf_anat]\n",
    "\n",
    "\n",
    "# Replace with your actual dfs\n",
    "dataframes_num = [diff_num05, diff_num1, diff_num2, diff_num3, diff_num4, diff_num5 ]\n",
    "dataframes_anat = [diff_anat05, diff_anat1, diff_anat2, diff_anat3, diff_anat4, diff_anat5 ]\n",
    "dataframes_pd = [diff_ratio05, diff_ratio1, diff_ratio2, diff_ratio3, diff_ratio4, diff_ratio5 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# RatioVar\n",
    "Wratio = [Wratio05, Wratio1, Wratio2, Wratio3, Wratio4, Wratio5]\n",
    "Nratio = [ratio05, ratio1, ratio2, ratio3, ratio4, ratio5]\n",
    "metrics = [\"degree_ratio\",\"clusteringcoef_ratio\",\"eigenvec_ratio\",\"betweeness_ratio\"]\n",
    "thresholds = [0.05,0.1,0.2,0.3,0.4,0.5] \n",
    "# ---- Helper: Cohen's d for independent samples ----\n",
    "def cohens_d(x, y):\n",
    "    nx = len(x)\n",
    "    ny = len(y)\n",
    "    pooled_sd = np.sqrt(((nx-1)*np.var(x, ddof=1) + (ny-1)*np.var(y, ddof=1)) / (nx+ny-2))\n",
    "    return (np.mean(x) - np.mean(y)) / pooled_sd\n",
    "\n",
    "# ---- Helper: get column with metric substring ----\n",
    "def get_metric_col(df, metric):\n",
    "    col = next((c for c in df.columns if metric in c), None)\n",
    "    if col is None:\n",
    "        raise KeyError(f\"No column containing '{metric}' found\")\n",
    "    return col\n",
    "\n",
    "# ---- Run KS-test + Cohen's d ----\n",
    "results = []\n",
    "\n",
    "for metric in metrics:\n",
    "    for t, df_num_nc, df_num_wc, df_anat_nc, df_anat_wc, df_rat_nc, df_rat_wc in zip(\n",
    "        thresholds, data_num_noconf, data_num_withconf, data_anat_noconf, data_anat_withconf, Nratio, Wratio\n",
    "    ):\n",
    "        # col_num = get_metric_col(df_num_nc, metric)\n",
    "        # col_anat = get_metric_col(df_anat_nc, metric)\n",
    "        col_ratio = get_metric_col(df_rat_nc, metric)\n",
    "\n",
    "    \n",
    "\n",
    "        # RATIO comparison\n",
    "        ratio_nc_vals = np.concatenate(df_rat_nc[col_ratio].values)\n",
    "        ratio_wc_vals = np.concatenate(df_rat_wc[col_ratio].values)\n",
    "        ks_stat_ratio, pval_ratio = ks_2samp(ratio_wc_vals, ratio_nc_vals)\n",
    "        d_ratio = cohens_d(ratio_wc_vals,ratio_nc_vals)\n",
    "\n",
    "        results.append({\n",
    "            'Metric': metric,\n",
    "            'Threshold': t,\n",
    "            'Cohen_d_ratio': d_ratio\n",
    "        })\n",
    "\n",
    "# ---- Convert to DataFrame ----\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
